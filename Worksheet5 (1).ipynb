{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TO DO TASK1"
      ],
      "metadata": {
        "id": "w1FguYLs7q6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the dataset (replace 'your_dataset.csv' with your actual dataset path)\n",
        "df = pd.read_csv('/content/drive/MyDrive/student.csv')\n",
        "\n",
        "# Observe the first few rows\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okT0ToQE7vmG",
        "outputId": "f2b568e3-6bec-4ca3-c0f5-07a4e9612cfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print top 5 rows\n",
        "print(\"Top 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Print bottom 5 rows\n",
        "print(\"Bottom 5 rows:\")\n",
        "print(df.tail())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8Jjj7d-77Ps",
        "outputId": "aec068a3-c9f2-48e6-dc0b-809af50c4a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 rows:\n",
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "Bottom 5 rows:\n",
            "     Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get concise information about the dataset\n",
        "print(\"Dataset Information:\")\n",
        "df.info()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA0xhuVD8ACK",
        "outputId": "3d6f0009-89ae-42ae-ec1d-8cd062f7cd9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get descriptive statistics for the numerical columns\n",
        "print(\"Descriptive Statistics:\")\n",
        "print(df.describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cds9LT0P8Xre",
        "outputId": "faec9e2b-6f5b-42b1-d48a-178179dfd29b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descriptive Statistics:\n",
            "              Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into Features (X) and Label (Y)\n",
        "X = df.drop(columns=['Math'])  # Features: Reading and Writing\n",
        "Y = df['Math']  # Label: Math\n",
        "\n",
        "# Print the shapes of X and Y to confirm\n",
        "print(\"Shape of X (Features):\", X.shape)\n",
        "print(\"Shape of Y (Label):\", Y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZowFY-258auN",
        "outputId": "44e4f1c3-a4e5-409d-ebb7-f3655a81b2cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X (Features): (1000, 2)\n",
            "Shape of Y (Label): (1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TO DO TASK2"
      ],
      "metadata": {
        "id": "AjholnB69GlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Number of features (d) - based on your dataset (Reading and Writing are features)\n",
        "d = X.shape[1]  # In this case, X has 2 features (Reading and Writing)\n",
        "\n",
        "# Initialize W with random values (just as an example)\n",
        "W = np.random.rand(d, 1)  # W is a d x 1 vector\n",
        "print(\"Weight Vector W:\")\n",
        "print(W)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq790EfW9Jjv",
        "outputId": "9c129470-ffbe-45a1-d1ec-a0f95003b357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight Vector W:\n",
            "[[0.87768267]\n",
            " [0.7553612 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature matrix X: 2 features (Reading and Writing) and 5 samples\n",
        "X = df[['Reading', 'Writing']].T  # Transpose to get the shape (d x n)\n",
        "X = X.values  # Convert to numpy array\n",
        "print(\"Feature Matrix X:\")\n",
        "print(X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3PWA7AR9TKk",
        "outputId": "aece6130-fa2e-482a-ed70-fbcf67616d27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Matrix X:\n",
            "[[68 81 80 ... 87 82 66]\n",
            " [63 72 78 ... 94 78 72]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output vector Y: target values for Math\n",
        "Y = df['Math'].values.reshape(-1, 1)  # Reshape to make it a column vector (n x 1)\n",
        "print(\"Output Vector Y:\")\n",
        "print(Y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJAYLVQ09cM6",
        "outputId": "63fb9dca-6ed8-446b-c8a1-1ae56df49a9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output Vector Y:\n",
            "[[ 48]\n",
            " [ 62]\n",
            " [ 79]\n",
            " [ 76]\n",
            " [ 59]\n",
            " [ 69]\n",
            " [ 70]\n",
            " [ 46]\n",
            " [ 61]\n",
            " [ 86]\n",
            " [ 62]\n",
            " [ 72]\n",
            " [ 56]\n",
            " [ 81]\n",
            " [ 61]\n",
            " [ 49]\n",
            " [ 60]\n",
            " [ 45]\n",
            " [ 71]\n",
            " [ 75]\n",
            " [ 66]\n",
            " [ 57]\n",
            " [ 67]\n",
            " [ 63]\n",
            " [ 68]\n",
            " [ 68]\n",
            " [ 62]\n",
            " [ 56]\n",
            " [ 82]\n",
            " [ 64]\n",
            " [ 71]\n",
            " [ 69]\n",
            " [ 64]\n",
            " [ 76]\n",
            " [ 61]\n",
            " [ 47]\n",
            " [ 50]\n",
            " [ 75]\n",
            " [ 69]\n",
            " [ 42]\n",
            " [ 73]\n",
            " [ 78]\n",
            " [ 65]\n",
            " [ 51]\n",
            " [ 61]\n",
            " [ 69]\n",
            " [ 51]\n",
            " [ 64]\n",
            " [ 90]\n",
            " [ 58]\n",
            " [100]\n",
            " [ 73]\n",
            " [ 62]\n",
            " [ 45]\n",
            " [ 47]\n",
            " [ 53]\n",
            " [ 62]\n",
            " [ 49]\n",
            " [ 77]\n",
            " [ 70]\n",
            " [ 60]\n",
            " [ 82]\n",
            " [100]\n",
            " [ 72]\n",
            " [ 62]\n",
            " [ 87]\n",
            " [ 56]\n",
            " [ 96]\n",
            " [ 68]\n",
            " [ 66]\n",
            " [ 68]\n",
            " [ 59]\n",
            " [ 52]\n",
            " [ 60]\n",
            " [ 60]\n",
            " [ 46]\n",
            " [ 62]\n",
            " [ 30]\n",
            " [ 83]\n",
            " [ 69]\n",
            " [ 82]\n",
            " [ 64]\n",
            " [ 57]\n",
            " [ 71]\n",
            " [ 37]\n",
            " [ 69]\n",
            " [ 72]\n",
            " [ 73]\n",
            " [ 70]\n",
            " [ 75]\n",
            " [ 54]\n",
            " [ 71]\n",
            " [ 60]\n",
            " [ 81]\n",
            " [ 58]\n",
            " [ 54]\n",
            " [ 49]\n",
            " [ 64]\n",
            " [ 84]\n",
            " [ 84]\n",
            " [ 33]\n",
            " [ 51]\n",
            " [ 68]\n",
            " [ 32]\n",
            " [ 43]\n",
            " [ 73]\n",
            " [ 82]\n",
            " [ 47]\n",
            " [ 74]\n",
            " [ 36]\n",
            " [ 58]\n",
            " [ 68]\n",
            " [ 92]\n",
            " [ 75]\n",
            " [ 43]\n",
            " [ 68]\n",
            " [ 49]\n",
            " [ 69]\n",
            " [ 91]\n",
            " [ 83]\n",
            " [ 65]\n",
            " [ 79]\n",
            " [ 74]\n",
            " [ 81]\n",
            " [ 41]\n",
            " [ 48]\n",
            " [ 31]\n",
            " [ 53]\n",
            " [ 87]\n",
            " [ 94]\n",
            " [ 77]\n",
            " [ 57]\n",
            " [ 64]\n",
            " [ 55]\n",
            " [ 61]\n",
            " [ 78]\n",
            " [ 94]\n",
            " [ 88]\n",
            " [ 77]\n",
            " [ 90]\n",
            " [ 55]\n",
            " [ 83]\n",
            " [ 68]\n",
            " [ 66]\n",
            " [ 56]\n",
            " [ 65]\n",
            " [ 84]\n",
            " [ 90]\n",
            " [ 80]\n",
            " [ 47]\n",
            " [ 67]\n",
            " [ 79]\n",
            " [ 68]\n",
            " [ 62]\n",
            " [ 62]\n",
            " [ 44]\n",
            " [ 74]\n",
            " [ 67]\n",
            " [ 46]\n",
            " [ 71]\n",
            " [ 73]\n",
            " [ 49]\n",
            " [ 42]\n",
            " [ 55]\n",
            " [ 73]\n",
            " [ 75]\n",
            " [ 96]\n",
            " [ 66]\n",
            " [ 56]\n",
            " [100]\n",
            " [ 53]\n",
            " [ 68]\n",
            " [ 72]\n",
            " [ 55]\n",
            " [ 59]\n",
            " [ 76]\n",
            " [ 84]\n",
            " [ 68]\n",
            " [ 61]\n",
            " [ 53]\n",
            " [ 73]\n",
            " [ 61]\n",
            " [ 73]\n",
            " [ 63]\n",
            " [ 51]\n",
            " [ 70]\n",
            " [ 69]\n",
            " [ 61]\n",
            " [ 88]\n",
            " [ 90]\n",
            " [ 60]\n",
            " [100]\n",
            " [ 68]\n",
            " [ 85]\n",
            " [ 41]\n",
            " [ 70]\n",
            " [ 91]\n",
            " [ 68]\n",
            " [ 63]\n",
            " [ 81]\n",
            " [ 89]\n",
            " [ 66]\n",
            " [ 59]\n",
            " [ 80]\n",
            " [ 87]\n",
            " [ 30]\n",
            " [ 63]\n",
            " [ 43]\n",
            " [ 73]\n",
            " [ 49]\n",
            " [ 65]\n",
            " [ 72]\n",
            " [ 83]\n",
            " [ 46]\n",
            " [ 65]\n",
            " [100]\n",
            " [ 40]\n",
            " [ 73]\n",
            " [ 58]\n",
            " [ 87]\n",
            " [ 61]\n",
            " [ 71]\n",
            " [ 72]\n",
            " [ 81]\n",
            " [ 67]\n",
            " [ 69]\n",
            " [ 53]\n",
            " [ 70]\n",
            " [ 56]\n",
            " [ 84]\n",
            " [ 44]\n",
            " [ 63]\n",
            " [ 82]\n",
            " [ 61]\n",
            " [ 68]\n",
            " [ 75]\n",
            " [ 78]\n",
            " [ 32]\n",
            " [ 62]\n",
            " [ 84]\n",
            " [ 65]\n",
            " [ 41]\n",
            " [ 57]\n",
            " [ 54]\n",
            " [ 58]\n",
            " [ 68]\n",
            " [ 53]\n",
            " [ 30]\n",
            " [ 63]\n",
            " [ 36]\n",
            " [ 85]\n",
            " [ 66]\n",
            " [ 45]\n",
            " [ 69]\n",
            " [ 71]\n",
            " [ 70]\n",
            " [ 31]\n",
            " [ 61]\n",
            " [ 73]\n",
            " [ 66]\n",
            " [ 63]\n",
            " [ 64]\n",
            " [ 63]\n",
            " [ 19]\n",
            " [ 64]\n",
            " [ 96]\n",
            " [ 59]\n",
            " [ 59]\n",
            " [ 75]\n",
            " [ 51]\n",
            " [100]\n",
            " [ 45]\n",
            " [ 67]\n",
            " [ 34]\n",
            " [ 82]\n",
            " [ 62]\n",
            " [ 59]\n",
            " [ 84]\n",
            " [ 80]\n",
            " [ 65]\n",
            " [ 71]\n",
            " [ 35]\n",
            " [ 46]\n",
            " [ 72]\n",
            " [ 70]\n",
            " [ 80]\n",
            " [ 72]\n",
            " [ 61]\n",
            " [ 39]\n",
            " [ 53]\n",
            " [ 80]\n",
            " [ 73]\n",
            " [ 68]\n",
            " [ 73]\n",
            " [ 57]\n",
            " [ 56]\n",
            " [ 46]\n",
            " [ 48]\n",
            " [ 78]\n",
            " [ 62]\n",
            " [ 89]\n",
            " [ 89]\n",
            " [ 65]\n",
            " [ 49]\n",
            " [ 65]\n",
            " [ 71]\n",
            " [ 70]\n",
            " [ 48]\n",
            " [ 75]\n",
            " [ 83]\n",
            " [ 51]\n",
            " [ 77]\n",
            " [ 51]\n",
            " [ 79]\n",
            " [ 73]\n",
            " [ 72]\n",
            " [ 68]\n",
            " [ 74]\n",
            " [ 58]\n",
            " [ 70]\n",
            " [ 52]\n",
            " [ 70]\n",
            " [ 71]\n",
            " [ 79]\n",
            " [ 71]\n",
            " [ 73]\n",
            " [ 89]\n",
            " [ 54]\n",
            " [ 90]\n",
            " [ 88]\n",
            " [ 72]\n",
            " [ 69]\n",
            " [ 61]\n",
            " [ 46]\n",
            " [ 98]\n",
            " [ 47]\n",
            " [ 71]\n",
            " [ 70]\n",
            " [ 83]\n",
            " [ 51]\n",
            " [ 85]\n",
            " [ 56]\n",
            " [ 98]\n",
            " [ 53]\n",
            " [ 47]\n",
            " [ 86]\n",
            " [ 93]\n",
            " [ 62]\n",
            " [ 66]\n",
            " [ 79]\n",
            " [ 56]\n",
            " [ 41]\n",
            " [ 79]\n",
            " [ 70]\n",
            " [ 89]\n",
            " [ 33]\n",
            " [ 78]\n",
            " [ 77]\n",
            " [ 58]\n",
            " [ 66]\n",
            " [ 72]\n",
            " [ 94]\n",
            " [ 58]\n",
            " [ 55]\n",
            " [ 65]\n",
            " [ 72]\n",
            " [ 46]\n",
            " [ 76]\n",
            " [ 67]\n",
            " [ 71]\n",
            " [ 88]\n",
            " [ 72]\n",
            " [ 66]\n",
            " [ 79]\n",
            " [ 73]\n",
            " [ 49]\n",
            " [ 58]\n",
            " [ 72]\n",
            " [ 78]\n",
            " [ 95]\n",
            " [ 87]\n",
            " [ 95]\n",
            " [ 64]\n",
            " [ 71]\n",
            " [ 80]\n",
            " [ 72]\n",
            " [ 64]\n",
            " [ 46]\n",
            " [ 63]\n",
            " [ 83]\n",
            " [ 61]\n",
            " [ 60]\n",
            " [ 47]\n",
            " [ 82]\n",
            " [ 59]\n",
            " [ 49]\n",
            " [ 80]\n",
            " [ 61]\n",
            " [ 72]\n",
            " [ 64]\n",
            " [ 92]\n",
            " [ 59]\n",
            " [ 69]\n",
            " [ 54]\n",
            " [ 74]\n",
            " [ 78]\n",
            " [ 69]\n",
            " [ 57]\n",
            " [ 55]\n",
            " [ 63]\n",
            " [ 89]\n",
            " [100]\n",
            " [ 60]\n",
            " [ 76]\n",
            " [ 73]\n",
            " [ 64]\n",
            " [ 40]\n",
            " [ 79]\n",
            " [ 90]\n",
            " [ 71]\n",
            " [ 47]\n",
            " [ 48]\n",
            " [ 78]\n",
            " [ 55]\n",
            " [ 76]\n",
            " [ 91]\n",
            " [ 39]\n",
            " [ 35]\n",
            " [ 60]\n",
            " [ 89]\n",
            " [ 82]\n",
            " [ 44]\n",
            " [ 89]\n",
            " [ 87]\n",
            " [ 45]\n",
            " [ 66]\n",
            " [ 81]\n",
            " [ 79]\n",
            " [ 71]\n",
            " [ 43]\n",
            " [ 79]\n",
            " [ 81]\n",
            " [ 64]\n",
            " [ 91]\n",
            " [ 62]\n",
            " [ 78]\n",
            " [ 90]\n",
            " [ 46]\n",
            " [ 81]\n",
            " [ 65]\n",
            " [ 58]\n",
            " [ 73]\n",
            " [ 73]\n",
            " [ 58]\n",
            " [ 46]\n",
            " [ 77]\n",
            " [ 42]\n",
            " [ 82]\n",
            " [ 64]\n",
            " [ 87]\n",
            " [ 60]\n",
            " [ 33]\n",
            " [ 60]\n",
            " [ 56]\n",
            " [ 73]\n",
            " [ 64]\n",
            " [ 64]\n",
            " [ 81]\n",
            " [ 60]\n",
            " [ 79]\n",
            " [ 76]\n",
            " [ 83]\n",
            " [ 65]\n",
            " [ 76]\n",
            " [ 78]\n",
            " [ 71]\n",
            " [ 73]\n",
            " [ 67]\n",
            " [ 64]\n",
            " [ 48]\n",
            " [ 82]\n",
            " [ 58]\n",
            " [ 74]\n",
            " [ 71]\n",
            " [ 60]\n",
            " [ 81]\n",
            " [ 47]\n",
            " [ 76]\n",
            " [ 55]\n",
            " [ 80]\n",
            " [ 79]\n",
            " [ 59]\n",
            " [ 62]\n",
            " [ 86]\n",
            " [ 89]\n",
            " [ 53]\n",
            " [ 70]\n",
            " [ 71]\n",
            " [ 56]\n",
            " [ 54]\n",
            " [ 64]\n",
            " [ 44]\n",
            " [ 61]\n",
            " [ 52]\n",
            " [ 56]\n",
            " [ 70]\n",
            " [ 69]\n",
            " [ 74]\n",
            " [ 73]\n",
            " [ 71]\n",
            " [ 82]\n",
            " [ 59]\n",
            " [ 50]\n",
            " [ 38]\n",
            " [ 81]\n",
            " [ 39]\n",
            " [ 68]\n",
            " [ 40]\n",
            " [ 80]\n",
            " [ 59]\n",
            " [ 78]\n",
            " [ 63]\n",
            " [ 71]\n",
            " [ 79]\n",
            " [ 42]\n",
            " [ 67]\n",
            " [ 76]\n",
            " [ 84]\n",
            " [ 74]\n",
            " [ 59]\n",
            " [ 57]\n",
            " [ 55]\n",
            " [ 55]\n",
            " [ 32]\n",
            " [ 95]\n",
            " [ 62]\n",
            " [ 72]\n",
            " [ 51]\n",
            " [ 46]\n",
            " [ 84]\n",
            " [ 68]\n",
            " [ 43]\n",
            " [ 73]\n",
            " [ 79]\n",
            " [ 81]\n",
            " [ 66]\n",
            " [ 93]\n",
            " [ 82]\n",
            " [ 78]\n",
            " [ 71]\n",
            " [ 67]\n",
            " [ 51]\n",
            " [ 89]\n",
            " [ 56]\n",
            " [ 67]\n",
            " [ 62]\n",
            " [ 72]\n",
            " [ 81]\n",
            " [ 67]\n",
            " [ 49]\n",
            " [ 64]\n",
            " [ 81]\n",
            " [ 99]\n",
            " [ 51]\n",
            " [ 78]\n",
            " [ 76]\n",
            " [ 64]\n",
            " [ 90]\n",
            " [ 67]\n",
            " [ 76]\n",
            " [ 43]\n",
            " [ 63]\n",
            " [ 82]\n",
            " [ 72]\n",
            " [ 69]\n",
            " [ 54]\n",
            " [ 54]\n",
            " [ 59]\n",
            " [ 66]\n",
            " [ 77]\n",
            " [ 70]\n",
            " [ 61]\n",
            " [ 75]\n",
            " [ 52]\n",
            " [ 77]\n",
            " [ 73]\n",
            " [ 55]\n",
            " [ 75]\n",
            " [ 45]\n",
            " [ 67]\n",
            " [ 57]\n",
            " [100]\n",
            " [ 67]\n",
            " [ 78]\n",
            " [ 56]\n",
            " [ 57]\n",
            " [ 83]\n",
            " [ 89]\n",
            " [100]\n",
            " [ 75]\n",
            " [ 60]\n",
            " [ 73]\n",
            " [ 81]\n",
            " [ 91]\n",
            " [ 61]\n",
            " [ 59]\n",
            " [ 65]\n",
            " [ 57]\n",
            " [ 57]\n",
            " [ 76]\n",
            " [ 77]\n",
            " [ 91]\n",
            " [ 68]\n",
            " [100]\n",
            " [ 57]\n",
            " [ 78]\n",
            " [ 80]\n",
            " [ 42]\n",
            " [ 66]\n",
            " [ 60]\n",
            " [ 67]\n",
            " [ 40]\n",
            " [ 64]\n",
            " [ 56]\n",
            " [ 61]\n",
            " [ 76]\n",
            " [ 90]\n",
            " [ 80]\n",
            " [ 72]\n",
            " [ 68]\n",
            " [ 40]\n",
            " [ 79]\n",
            " [ 77]\n",
            " [ 83]\n",
            " [ 96]\n",
            " [100]\n",
            " [ 72]\n",
            " [ 79]\n",
            " [ 80]\n",
            " [ 67]\n",
            " [ 53]\n",
            " [ 66]\n",
            " [ 91]\n",
            " [ 52]\n",
            " [ 97]\n",
            " [ 63]\n",
            " [ 69]\n",
            " [ 65]\n",
            " [ 65]\n",
            " [ 84]\n",
            " [ 79]\n",
            " [ 73]\n",
            " [ 92]\n",
            " [ 60]\n",
            " [ 84]\n",
            " [ 82]\n",
            " [ 44]\n",
            " [ 90]\n",
            " [ 69]\n",
            " [ 72]\n",
            " [ 69]\n",
            " [ 57]\n",
            " [ 93]\n",
            " [ 69]\n",
            " [ 75]\n",
            " [ 45]\n",
            " [ 46]\n",
            " [ 90]\n",
            " [ 66]\n",
            " [ 57]\n",
            " [ 65]\n",
            " [ 87]\n",
            " [ 39]\n",
            " [ 87]\n",
            " [ 77]\n",
            " [ 57]\n",
            " [ 59]\n",
            " [ 74]\n",
            " [ 60]\n",
            " [ 75]\n",
            " [ 95]\n",
            " [ 74]\n",
            " [ 53]\n",
            " [ 56]\n",
            " [ 91]\n",
            " [ 74]\n",
            " [ 47]\n",
            " [ 95]\n",
            " [ 49]\n",
            " [ 66]\n",
            " [100]\n",
            " [ 33]\n",
            " [ 99]\n",
            " [ 39]\n",
            " [ 56]\n",
            " [ 36]\n",
            " [ 58]\n",
            " [ 52]\n",
            " [ 59]\n",
            " [ 67]\n",
            " [ 55]\n",
            " [ 40]\n",
            " [ 32]\n",
            " [ 91]\n",
            " [ 31]\n",
            " [ 54]\n",
            " [ 69]\n",
            " [ 55]\n",
            " [ 73]\n",
            " [ 98]\n",
            " [ 60]\n",
            " [ 26]\n",
            " [ 95]\n",
            " [ 76]\n",
            " [ 53]\n",
            " [ 67]\n",
            " [ 80]\n",
            " [ 76]\n",
            " [ 74]\n",
            " [ 67]\n",
            " [ 81]\n",
            " [ 95]\n",
            " [ 58]\n",
            " [ 80]\n",
            " [ 83]\n",
            " [ 52]\n",
            " [ 69]\n",
            " [ 59]\n",
            " [ 62]\n",
            " [ 70]\n",
            " [ 65]\n",
            " [ 61]\n",
            " [ 42]\n",
            " [ 69]\n",
            " [ 76]\n",
            " [ 83]\n",
            " [ 62]\n",
            " [ 42]\n",
            " [ 64]\n",
            " [ 61]\n",
            " [ 69]\n",
            " [ 77]\n",
            " [ 80]\n",
            " [ 93]\n",
            " [ 63]\n",
            " [100]\n",
            " [ 88]\n",
            " [ 76]\n",
            " [ 72]\n",
            " [ 84]\n",
            " [ 86]\n",
            " [ 59]\n",
            " [ 77]\n",
            " [ 53]\n",
            " [ 87]\n",
            " [ 72]\n",
            " [ 67]\n",
            " [ 65]\n",
            " [ 84]\n",
            " [ 77]\n",
            " [ 77]\n",
            " [ 77]\n",
            " [ 45]\n",
            " [ 61]\n",
            " [ 61]\n",
            " [ 62]\n",
            " [ 68]\n",
            " [ 62]\n",
            " [ 49]\n",
            " [ 13]\n",
            " [ 55]\n",
            " [ 85]\n",
            " [ 78]\n",
            " [ 69]\n",
            " [ 69]\n",
            " [ 45]\n",
            " [ 67]\n",
            " [ 68]\n",
            " [ 81]\n",
            " [ 73]\n",
            " [ 53]\n",
            " [ 80]\n",
            " [ 56]\n",
            " [ 82]\n",
            " [ 69]\n",
            " [ 76]\n",
            " [ 44]\n",
            " [ 25]\n",
            " [ 82]\n",
            " [ 55]\n",
            " [ 65]\n",
            " [ 79]\n",
            " [ 95]\n",
            " [ 62]\n",
            " [ 80]\n",
            " [ 93]\n",
            " [ 57]\n",
            " [ 54]\n",
            " [100]\n",
            " [ 67]\n",
            " [ 47]\n",
            " [ 69]\n",
            " [ 84]\n",
            " [ 50]\n",
            " [ 79]\n",
            " [ 50]\n",
            " [ 48]\n",
            " [ 56]\n",
            " [ 72]\n",
            " [ 47]\n",
            " [ 68]\n",
            " [ 79]\n",
            " [ 74]\n",
            " [ 37]\n",
            " [ 73]\n",
            " [ 92]\n",
            " [ 75]\n",
            " [ 61]\n",
            " [ 56]\n",
            " [ 52]\n",
            " [ 96]\n",
            " [ 59]\n",
            " [ 67]\n",
            " [ 89]\n",
            " [ 50]\n",
            " [ 51]\n",
            " [ 76]\n",
            " [ 80]\n",
            " [ 68]\n",
            " [ 46]\n",
            " [100]\n",
            " [ 44]\n",
            " [ 48]\n",
            " [ 67]\n",
            " [ 88]\n",
            " [ 74]\n",
            " [ 82]\n",
            " [ 77]\n",
            " [ 56]\n",
            " [ 96]\n",
            " [ 67]\n",
            " [ 60]\n",
            " [ 63]\n",
            " [ 77]\n",
            " [ 50]\n",
            " [ 44]\n",
            " [ 64]\n",
            " [ 50]\n",
            " [ 82]\n",
            " [ 91]\n",
            " [ 31]\n",
            " [ 67]\n",
            " [ 90]\n",
            " [ 89]\n",
            " [ 61]\n",
            " [ 64]\n",
            " [ 69]\n",
            " [ 78]\n",
            " [ 58]\n",
            " [ 68]\n",
            " [ 64]\n",
            " [ 85]\n",
            " [ 68]\n",
            " [ 63]\n",
            " [ 68]\n",
            " [ 34]\n",
            " [ 77]\n",
            " [ 61]\n",
            " [ 53]\n",
            " [ 80]\n",
            " [ 62]\n",
            " [ 79]\n",
            " [ 83]\n",
            " [ 65]\n",
            " [ 58]\n",
            " [ 73]\n",
            " [ 94]\n",
            " [ 86]\n",
            " [ 76]\n",
            " [ 58]\n",
            " [ 61]\n",
            " [ 58]\n",
            " [ 64]\n",
            " [ 51]\n",
            " [ 82]\n",
            " [ 56]\n",
            " [ 65]\n",
            " [ 76]\n",
            " [ 48]\n",
            " [ 85]\n",
            " [ 68]\n",
            " [ 85]\n",
            " [ 62]\n",
            " [ 75]\n",
            " [ 38]\n",
            " [ 63]\n",
            " [ 68]\n",
            " [ 62]\n",
            " [ 73]\n",
            " [ 65]\n",
            " [ 77]\n",
            " [ 63]\n",
            " [ 83]\n",
            " [ 55]\n",
            " [ 87]\n",
            " [ 95]\n",
            " [ 48]\n",
            " [ 36]\n",
            " [ 84]\n",
            " [ 91]\n",
            " [ 79]\n",
            " [ 51]\n",
            " [ 76]\n",
            " [ 82]\n",
            " [ 71]\n",
            " [ 52]\n",
            " [ 60]\n",
            " [ 64]\n",
            " [ 68]\n",
            " [ 37]\n",
            " [ 67]\n",
            " [ 51]\n",
            " [ 78]\n",
            " [ 75]\n",
            " [ 60]\n",
            " [ 67]\n",
            " [ 51]\n",
            " [ 78]\n",
            " [ 64]\n",
            " [ 59]\n",
            " [ 85]\n",
            " [ 74]\n",
            " [ 40]\n",
            " [ 94]\n",
            " [ 58]\n",
            " [ 68]\n",
            " [ 83]\n",
            " [ 83]\n",
            " [ 68]\n",
            " [ 74]\n",
            " [ 85]\n",
            " [ 65]\n",
            " [ 68]\n",
            " [ 75]\n",
            " [ 52]\n",
            " [ 74]\n",
            " [ 60]\n",
            " [ 51]\n",
            " [ 64]\n",
            " [ 76]\n",
            " [ 45]\n",
            " [ 55]\n",
            " [ 45]\n",
            " [ 86]\n",
            " [ 85]\n",
            " [ 58]\n",
            " [ 72]\n",
            " [ 59]\n",
            " [ 60]\n",
            " [ 70]\n",
            " [ 79]\n",
            " [ 77]\n",
            " [ 77]\n",
            " [ 66]\n",
            " [ 68]\n",
            " [ 65]\n",
            " [ 70]\n",
            " [ 70]\n",
            " [ 55]\n",
            " [ 81]\n",
            " [ 62]\n",
            " [ 81]\n",
            " [ 71]\n",
            " [ 42]\n",
            " [ 62]\n",
            " [ 57]\n",
            " [ 80]\n",
            " [ 78]\n",
            " [ 71]\n",
            " [ 86]\n",
            " [ 76]\n",
            " [ 45]\n",
            " [ 71]\n",
            " [ 64]\n",
            " [ 69]\n",
            " [ 84]\n",
            " [ 65]\n",
            " [ 55]\n",
            " [ 64]\n",
            " [ 84]\n",
            " [ 52]\n",
            " [ 67]\n",
            " [ 77]\n",
            " [ 64]\n",
            " [ 58]\n",
            " [ 72]\n",
            " [ 73]\n",
            " [ 89]\n",
            " [ 83]\n",
            " [ 66]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the predicted Y values\n",
        "Y_pred = np.dot(W.T, X)\n",
        "print(\"Predicted Values (Y_pred):\")\n",
        "print(Y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KN-9HU9S9oPW",
        "outputId": "c2495883-ca38-42a5-8bbc-9ce700f1f197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Values (Y_pred):\n",
            "[[107.27017755 125.47830313 129.13278767 132.5211969  103.00408566\n",
            "  137.93104678 136.42032438  73.09857763 128.88814473 126.62206112\n",
            "  108.90322143 145.21858349  72.60929174 126.62206112 114.31307132\n",
            "   73.36465295 119.7229212   75.36466124 143.19714282 120.21220709\n",
            "   90.93973879 107.78089581 119.84524267  82.65219794 138.93105093\n",
            "  141.68642042 109.29161822 115.68003988 126.62206112 130.88815302\n",
            "  123.47829484 141.68642042 119.33452442 109.65858263  70.73160492\n",
            "  103.39248244  84.28524182 127.25510085 116.31307961  73.36465295\n",
            "  141.31945601 120.45685003 100.49335911  83.04059473 114.06842837\n",
            "  119.84524267  84.28524182 123.11133043 163.30438759 126.13277524\n",
            "  163.30438759 143.83018255 130.01047035  81.01915407  81.6521938\n",
            "   76.24234391  84.52988476  76.36466538 111.16930504 130.01047035\n",
            "   92.08349678 149.48467538 155.50613263 131.76583569  69.46552546\n",
            "  144.95250817 114.82378958 135.5426417   98.73799376  94.7165448\n",
            "   98.86031523  93.32814387  78.38610605 102.12640298 108.41393554\n",
            "   91.08349264 105.63713368  77.50842337 122.60061217 104.63712953\n",
            "  132.39887542 130.76583155 118.33452027 127.25510085  74.36465709\n",
            "  106.78089167 143.58553961 127.4997438  130.5211886  138.44176504\n",
            "   84.79596008 115.43539693 122.86668748 119.60059973 105.51481221\n",
            "  116.57915492  99.7379979  105.90320899 163.30438759 136.90961026\n",
            "   46.84755414  85.04060302 127.13277938  92.20581826  65.19943357\n",
            "  128.255105   117.19076228  75.63073655 125.86669992  73.09857763\n",
            "  115.06843252 130.13279182 142.70785694 144.8301867   83.67363446\n",
            "  134.15424077  91.45045705 122.60061217 139.68641213 126.25509671\n",
            "  116.57915492 126.11134286 138.68640798  99.34960112  83.04059473\n",
            "   75.36466124  65.19943357  93.71654066 112.4353845  134.0319193\n",
            "  122.60061217 130.01047035 110.29162236  89.18437345  90.57277438\n",
            "  126.86670407 131.27654981 129.88814887 153.77219965 131.13279596\n",
            "  102.4933674  159.40526011 118.96756    106.63713782 101.10496646\n",
            "  141.07481306 107.02553461 163.30438759 127.25510085  95.96119189\n",
            "   99.61567643 137.29800705 118.2121988   94.59422333  94.32814802\n",
            "   77.63074484 140.93105922 120.84524682 101.61568472 119.96756415\n",
            "  136.29800291 105.12641542  67.58783864 116.68004402 107.5148205\n",
            "  122.98900895 138.93105093 120.08988562 116.57915492 152.87308461\n",
            "   99.37103349 121.08988976 102.24872445  97.22727135 117.82380201\n",
            "  146.46323058 133.52120104 103.51480392 123.60061631 103.88176833\n",
            "  110.04697942  92.4504612  112.19074155 102.88176418 112.04698771\n",
            "  137.80872531 127.37742232 140.5640948  133.78727636 135.03192344\n",
            "   96.22726721 146.09626616  96.7165531  112.55770597  89.81741318\n",
            "  126.25509671 138.2980112  105.00409395 125.62205698 123.2336519\n",
            "  148.2400283  118.96756    107.53625287 112.4353845  139.68641213\n",
            "   77.75306631  95.59422748  78.63074899 128.39885884 101.7380062\n",
            "   93.08350093  96.34958868 123.9890131   96.73798547 110.16930089\n",
            "  158.16061302  76.6307407  151.24004073  97.4719143  134.15424077\n",
            "   81.26379701 121.60060802  99.86031938 134.17567314 134.7872805\n",
            "  129.0104662  109.53626116 115.43539693 106.90321314 111.02555119\n",
            "   98.1263864  102.00408151 121.08988976 121.84525097 129.25510914\n",
            "  140.68641627 134.66495903  69.46552546 116.70147639 127.62206527\n",
            "  123.2336519   58.42261512 119.08988147  74.60930003 104.63712953\n",
            "  127.86670821  81.40755085  52.3797255  104.39248659  72.60929174\n",
            "  148.21859592 102.00408151  85.04060302 136.54264585 141.31945601\n",
            "  127.62206527  56.37974208 119.57916736 124.62205283 105.14784779\n",
            "  103.12640713 103.00408566  78.50842752  29.51711124 105.27016926\n",
            "  131.13279596 126.86670407 112.16930918 107.75946344 101.37104178\n",
            "  154.50612848  72.34321643 108.65857849  71.09856934 123.84525926\n",
            "  101.00407737  93.32814387 135.17567729 107.90321728 112.55770597\n",
            "  111.41394798  89.18437345  81.77451527 106.78089167 123.2336519\n",
            "  148.11770682 122.35596922  74.87537535  96.22726721  96.20583484\n",
            "  110.04697942 126.49973965 104.759451   110.41394383  85.91828569\n",
            "   86.6736469   78.63074899  68.58784279 126.62206112  94.83886628\n",
            "  158.03829155 143.3194643  106.6585702   76.6307407  114.06842837\n",
            "   86.91828984 113.1907457   81.04058644 124.60062046 132.76583984\n",
            "   92.32813973 151.75075899 104.14784365 137.42032852 124.23365604\n",
            "  126.25509671 110.27018999 123.60061631 116.45683345 103.88176833\n",
            "   87.4290081  108.29161407 118.08987733 105.88177662  99.98264085\n",
            "  120.33452856 137.68640384 119.96756415 132.4203078  124.47829899\n",
            "  132.64351837 118.84523853 112.94610276  90.69509585 155.26148969\n",
            "  112.4353845  119.7229212   98.10495403 149.48467538  92.20581826\n",
            "  132.90959368  97.22727135 150.99539779 110.65858678  97.34959283\n",
            "  127.25510085 157.64989476 119.7229212  104.51480806 120.45685003\n",
            "  130.64351008  52.13508256 156.89453356 103.63712539 148.48467124\n",
            "   55.52349178 119.33452442 135.78728465  95.71654895 129.0104662\n",
            "  138.05336825 133.90959783  88.1843693   69.97624372  91.6951\n",
            "  100.37103763  79.14146725 120.84524682 103.37105007 108.14786023\n",
            "  139.0533724  104.759451   114.94611105 121.96757244 110.53626531\n",
            "  103.51480392  97.83887871 122.11132628 134.7872805  137.93104678\n",
            "  130.49975623 132.03191101 115.94611519 134.66495903 120.47828241\n",
            "  106.90321314 120.72292535  89.30669492 112.80234891 137.17568558\n",
            "  104.14784365 119.57916736  70.46552961 141.58553132  83.01916236\n",
            "   93.83886213 116.43540108 119.57916736 139.93105507 107.14785608\n",
            "  163.30438759  94.0620727  123.9890131   99.22727965 108.53625701\n",
            "  131.52119275 121.72292949  72.22089496 110.29162236  98.9826367\n",
            "  145.70786937 147.21859178 100.98264499 130.64351008  98.2272755\n",
            "   87.55132957  59.40118689 133.66495488 157.52757329 117.70148054\n",
            "  104.759451   100.86032352 121.21221124  72.22089496 105.14784779\n",
            "  161.54902225  85.04060302  62.4226317  134.90960197 148.2400283\n",
            "  117.57915907  62.93334996 125.13277109 129.0104662   91.57277852\n",
            "  122.4782907  144.34090081 156.89453356 110.53626531  63.68871116\n",
            "  141.44177748 137.80872531  86.42900395 153.62844581 117.06844081\n",
            "  133.66495488 130.01047035  94.7165448  113.9461069   97.10494988\n",
            "   78.87539193 108.53625701 115.31307546  94.20582655  84.93971392\n",
            "  141.07481306  72.09857348 136.15424906 129.27654152 108.41393554\n",
            "  105.14784779  85.04060302  74.60930003  69.95481135 108.29161407\n",
            "  143.46321814 117.4568376  117.31308375 100.10496232 139.29801534\n",
            "  105.51481221 126.25509671 130.88815302 101.75943857 121.72292949\n",
            "  135.03192344 101.49336325  90.96117117 102.00408151  97.49334667\n",
            "  125.47830313  90.30669906 115.94611519 109.0255429  131.27654981\n",
            "  125.86669992  71.58785523 129.88814887  81.01915407 115.19075399\n",
            "  139.07480477 120.08988562  94.7165448  162.42670492 134.15424077\n",
            "  127.74438674 114.06842837 137.78729294  84.04059888  90.57277438\n",
            "  110.53626531  66.46551303  96.34958868 104.0040898   86.40757158\n",
            "  104.759451   125.01044962 146.09626616 126.74438259 119.33452442\n",
            "  111.80234477  91.32813558  92.32813973  74.09858178 127.25510085\n",
            "   73.12001     95.71654895  76.75306217 131.76583569  80.50843581\n",
            "  127.86670821 112.68002744  95.47190601 130.03190272  80.50843581\n",
            "   92.57278267 121.96757244 120.84524682 118.08987733  88.9397305\n",
            "   78.26378457  94.45046949 138.17568972  41.19306131 163.30438759\n",
            "  112.04698771 140.80873775  83.89684503  73.73161736 127.86670821\n",
            "  100.12639469  64.17799705 104.14784365 141.19713453 125.74437845\n",
            "  150.36235806 122.60061217 159.40526011 134.7872805  130.88815302\n",
            "  119.21220294  90.18437759 150.36235806 110.41394383 111.16930504\n",
            "  108.04697113 123.47829484 115.06843252 101.37104178  74.99769682\n",
            "  119.7229212  132.66495074 149.24003244  66.07711624 129.64350593\n",
            "  138.68640798  90.06205612 141.58553132 106.6585702  160.28294279\n",
            "   83.65220209 111.55770182 113.70146396 118.45684174 120.33452856\n",
            "  106.51481635 110.29162236  89.18437345 116.45683345 106.39249488\n",
            "  140.05337654  96.59423162 110.92466209  89.30669492 143.34089667\n",
            "  116.57915492 117.4568376  123.35597337  64.05567558 113.31306717\n",
            "  101.24872031 163.30438759  82.52987647 112.94610276 101.49336325\n",
            "  113.70146396 134.66495903 141.68642042 161.54902225 107.65857434\n",
            "  118.57916321 132.76583984 112.92467038 154.26148554  94.57279096\n",
            "  116.82379787 115.57915078 101.49336325 108.53625701 120.23363946\n",
            "  125.98902139 154.26148554 117.06844081 126.25509671  81.01915407\n",
            "  107.5148205  133.90959783  61.05566314 116.94611934 115.19075399\n",
            "  114.43539279  52.25740403 129.88814887  86.55132543 106.88178077\n",
            "  132.03191101 162.42670492 130.64351008 133.2765581  138.2980112\n",
            "   90.57277438 135.5426417  138.80872946 151.60700515 135.78728465\n",
            "  156.89453356 108.14786023 116.06843666 141.83017426  99.86031938\n",
            "   88.81740903 101.24872031 156.89453356 105.51481221 130.39886713\n",
            "  113.55771011 101.7380062  126.11134286 111.29162651 133.03191515\n",
            "  150.87307632 114.31307132 130.88815302  98.49335082 154.75077143\n",
            "  119.57916736 101.24872031 128.13278353 102.63712124 126.98902554\n",
            "   94.08350507 106.14785194 163.30438759 114.31307132 115.68003988\n",
            "   83.79595593 118.45684174 148.72931418  92.32813973 105.27016926\n",
            "   80.77451112 159.52758158  62.93334996 132.01047864 132.78727221\n",
            "  112.70145981  94.20582655 127.3988547  114.31307132 121.84525097\n",
            "  131.52119275 136.42032438  86.55132543 114.68003573 163.30438759\n",
            "  111.16930504  79.75307461 150.62843337  77.50842337 129.25510914\n",
            "  163.30438759  68.83248573 146.85162736  94.20582655  66.95479891\n",
            "   37.3153662  104.63712953  96.34958868 104.63712953 129.25510914\n",
            "   78.63074899  81.77451527  72.48697027 150.99539779  65.44407651\n",
            "   98.2272755  142.07481721  96.59423162 126.62206112 161.54902225\n",
            "  105.27016926  63.42263585 143.83018255 134.90960197 101.12639884\n",
            "  116.19075814 124.35597752 114.31307132 146.60698442 105.51481221\n",
            "  128.76582326 136.66496732 101.12639884 138.56408651 132.88816131\n",
            "   94.32814802 150.607001   104.63712953 124.98901725 122.86668748\n",
            "  125.98902139  86.04060717  64.93335825 141.80874189 138.17568972\n",
            "  143.19714282 129.52118446  63.44406822 129.52118446  81.38611848\n",
            "   97.4719143  137.56408237 134.90960197 161.79366519 101.37104178\n",
            "  156.89453356 134.54263756 112.4353845   88.42901224 118.57916321\n",
            "  149.60699686 119.84524267 105.63713368 104.63712953 136.05335996\n",
            "  116.94611934 123.11133043 111.92466624 132.39887542 103.27016097\n",
            "  104.14784365 145.34090496  94.47190186 127.25510085 129.7658274\n",
            "  127.37742232 112.55770597 121.08988976  78.12003073  27.25102763\n",
            "   94.32814802 151.11771926 118.57916321 109.65858263  97.34959283\n",
            "   99.34960112 115.55771841 101.88176004 129.64350593 104.73801863\n",
            "   74.36465709 143.95250403  92.81742561 121.60060802 136.66496732\n",
            "   99.49335496  73.85393883  43.94843081 119.33452442  91.57277852\n",
            "  106.90321314 115.31307546 140.44177333 110.29162236 130.01047035\n",
            "  136.17568143 111.29162651  80.26379286 163.30438759 102.88176418\n",
            "  102.12640298  99.61567643 113.55771011  93.9611836  128.49974794\n",
            "   84.91828155  86.40757158  90.30669906 102.24872445  76.12002244\n",
            "  106.88178077 137.54264999 116.68004402  91.81742147  96.98262841\n",
            "  142.58553547 126.11134286  99.49335496 112.04698771  86.91828984\n",
            "  141.56409895 118.70148469  91.93974294 155.50613263 111.41394798\n",
            "  107.90321728 113.80235306 123.72293778 103.39248244  98.61567229\n",
            "  162.54902639 100.37103763  90.06205612 103.61569301 131.76583569\n",
            "   93.2058224  141.19713453 129.88814887 104.63712953 141.19713453\n",
            "  137.80872531 122.4782907  112.55770597 110.27018999  80.8968326\n",
            "   71.34321228 122.98900895  89.45044876 125.11133872 143.58553961\n",
            "   60.78958783  97.10494988 132.39887542 145.83019085 126.23366433\n",
            "   99.98264085 112.68002744 126.11134286  86.6736469   96.47191015\n",
            "  126.37741818 135.17567729 102.24872445 111.04698356 111.16930504\n",
            "   76.6307407  110.53626531 112.19074155  93.9611836  120.96756829\n",
            "   94.83886628 126.25509671 135.29799876 126.13277524  98.73799376\n",
            "  116.19075814 163.30438759 156.40524768 147.60698857 118.45684174\n",
            "   98.34959697  70.97624787  91.08349264 107.39249903 145.72930175\n",
            "  105.39249073 101.86032767 119.33452442  70.58785108 143.19714282\n",
            "   96.22726721 134.66495903 109.90322558 116.45683345  74.48697856\n",
            "  103.12640713 114.7014681  113.68003159 130.88815302  86.91828984\n",
            "  117.82380201 117.19076228 136.54264585  79.50843166 157.64989476\n",
            "  161.54902225  70.97624787  73.9762603  153.38380287 127.25510085\n",
            "  112.04698771  90.18437759 142.07481721 125.11133872 103.12640713\n",
            "   65.07711209 100.61568058  97.4719143  108.02553876  85.79596422\n",
            "  108.41393554  99.98264085 144.85161907 125.98902139 131.88815716\n",
            "  116.55772255  99.37103349 142.58553547  73.73161736 107.90321728\n",
            "  154.26148554 136.05335996  55.8904562  154.26148554  80.52986818\n",
            "  113.41395627 136.80872116 116.94611934  96.34958868 137.17568558\n",
            "  131.64351422 120.47828241 119.84524267 107.78089581 114.43539279\n",
            "  142.31946015 100.24871616  76.38609775  92.08349678 136.78728879\n",
            "   73.24233147  91.57277852 104.0040898  147.34091325 133.64352251\n",
            "   95.20583069 106.02553046 110.16930089  89.93973465 120.5791715\n",
            "  123.47829484 134.0319193  115.82379372 104.0040898  118.84523853\n",
            "  106.39249488 107.14785608 118.33452027 104.759451   151.11771926\n",
            "  113.55771011 116.06843666 117.5577267   90.93973879 119.08988147\n",
            "  103.12640713 131.52119275 135.29799876 105.63713368 133.2765581\n",
            "  137.29800705  89.69509171 120.60060388  91.93974294  92.06206441\n",
            "  121.72292949 100.98264499 103.75944686 102.24872445 146.46323058\n",
            "   97.10494988  84.67363861 153.24004902 106.78089167  87.93972636\n",
            "  117.82380201 143.46321814 147.36234562 130.88815302 112.31306303]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO TASK3"
      ],
      "metadata": {
        "id": "Fp-ILeGc94x9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming the data is already loaded in `df` as:\n",
        "# df = pd.DataFrame({'Math': [48, 62, 79, 76, 59], 'Reading': [68, 81, 80, 83, 64], 'Writing': [63, 72, 78, 79, 62]})\n",
        "\n",
        "# Features (X) and Target (Y)\n",
        "X = df[['Reading', 'Writing']].values  # Features: Reading and Writing\n",
        "Y = df['Math'].values.reshape(-1, 1)  # Target: Math scores\n",
        "\n",
        "# Step 1: Split the dataset into training and test sets (80% training, 20% testing)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the splits to confirm\n",
        "print(\"Training Features (X_train) shape:\", X_train.shape)\n",
        "print(\"Testing Features (X_test) shape:\", X_test.shape)\n",
        "print(\"Training Labels (Y_train) shape:\", Y_train.shape)\n",
        "print(\"Testing Labels (Y_test) shape:\", Y_test.shape)\n",
        "\n",
        "# Step 2: Create the Weight Vector W (d x 1) for linear regression\n",
        "d = X.shape[1]  # Number of features (Reading and Writing)\n",
        "W = np.random.rand(d, 1)  # Initialize W with random values\n",
        "\n",
        "# Step 3: Compute the predicted Y values (Y_pred) using W^T * X\n",
        "Y_pred_train = np.dot(X_train, W)  # Prediction for training data\n",
        "Y_pred_test = np.dot(X_test, W)  # Prediction for test data\n",
        "\n",
        "# Print the matrices and predictions\n",
        "print(\"\\nFeature Matrix X (Training):\")\n",
        "print(X_train)\n",
        "print(\"Feature Matrix X (Testing):\")\n",
        "print(X_test)\n",
        "\n",
        "print(\"\\nWeight Vector W:\")\n",
        "print(W)\n",
        "\n",
        "print(\"\\nPredicted Y values (Training):\")\n",
        "print(Y_pred_train)\n",
        "\n",
        "print(\"\\nPredicted Y values (Testing):\")\n",
        "print(Y_pred_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rc3kOlkM99X7",
        "outputId": "5bc78a8f-4337-4b19-94a7-46f27ae417ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Features (X_train) shape: (800, 2)\n",
            "Testing Features (X_test) shape: (200, 2)\n",
            "Training Labels (Y_train) shape: (800, 1)\n",
            "Testing Labels (Y_test) shape: (200, 1)\n",
            "\n",
            "Feature Matrix X (Training):\n",
            "[[82 78]\n",
            " [70 67]\n",
            " [21 25]\n",
            " ...\n",
            " [76 79]\n",
            " [75 75]\n",
            " [76 80]]\n",
            "Feature Matrix X (Testing):\n",
            "[[ 69  69]\n",
            " [ 37  41]\n",
            " [ 62  57]\n",
            " [ 59  56]\n",
            " [ 92  88]\n",
            " [ 70  70]\n",
            " [ 99 100]\n",
            " [ 50  40]\n",
            " [ 60  58]\n",
            " [ 79  82]\n",
            " [ 87  81]\n",
            " [ 67  62]\n",
            " [ 69  63]\n",
            " [ 58  53]\n",
            " [ 59  59]\n",
            " [ 56  50]\n",
            " [ 71  65]\n",
            " [ 69  62]\n",
            " [ 89  89]\n",
            " [ 65  62]\n",
            " [ 82  78]\n",
            " [ 74  70]\n",
            " [ 66  62]\n",
            " [ 74  74]\n",
            " [ 57  57]\n",
            " [ 88  84]\n",
            " [ 53  51]\n",
            " [ 66  57]\n",
            " [ 85  82]\n",
            " [ 44  50]\n",
            " [ 97  96]\n",
            " [ 84  76]\n",
            " [ 32  32]\n",
            " [ 55  56]\n",
            " [ 87  83]\n",
            " [ 76  78]\n",
            " [ 89  96]\n",
            " [ 85  91]\n",
            " [ 75  68]\n",
            " [ 81  78]\n",
            " [ 76  87]\n",
            " [ 62  60]\n",
            " [ 61  67]\n",
            " [ 72  74]\n",
            " [ 59  57]\n",
            " [ 80  86]\n",
            " [ 84  77]\n",
            " [ 85  77]\n",
            " [ 39  39]\n",
            " [ 80  79]\n",
            " [ 54  48]\n",
            " [ 73  72]\n",
            " [ 64  62]\n",
            " [ 69  74]\n",
            " [ 52  49]\n",
            " [ 77  74]\n",
            " [ 36  39]\n",
            " [ 69  64]\n",
            " [ 74  75]\n",
            " [ 89  84]\n",
            " [ 50  46]\n",
            " [ 50  50]\n",
            " [ 61  67]\n",
            " [ 66  58]\n",
            " [ 63  52]\n",
            " [ 89  91]\n",
            " [100 100]\n",
            " [ 70  65]\n",
            " [ 61  60]\n",
            " [ 86  87]\n",
            " [ 44  49]\n",
            " [ 99 100]\n",
            " [ 62  62]\n",
            " [ 81  88]\n",
            " [ 66  62]\n",
            " [ 64  53]\n",
            " [ 82  78]\n",
            " [ 44  46]\n",
            " [ 68  71]\n",
            " [ 45  55]\n",
            " [ 76  73]\n",
            " [ 68  73]\n",
            " [ 60  64]\n",
            " [ 83  83]\n",
            " [ 69  74]\n",
            " [ 83  82]\n",
            " [ 82  78]\n",
            " [ 61  63]\n",
            " [ 65  63]\n",
            " [ 73  74]\n",
            " [ 71  71]\n",
            " [ 66  67]\n",
            " [ 58  59]\n",
            " [ 62  57]\n",
            " [ 32  21]\n",
            " [ 87  90]\n",
            " [ 79  80]\n",
            " [ 79  81]\n",
            " [ 83  78]\n",
            " [ 72  67]\n",
            " [ 78  74]\n",
            " [ 31  33]\n",
            " [ 81  87]\n",
            " [ 41  27]\n",
            " [ 97  95]\n",
            " [ 78  82]\n",
            " [ 69  67]\n",
            " [ 78  69]\n",
            " [ 70  73]\n",
            " [ 62  57]\n",
            " [ 70  75]\n",
            " [ 98  92]\n",
            " [ 75  74]\n",
            " [ 80  74]\n",
            " [ 87  89]\n",
            " [ 72  82]\n",
            " [ 70  67]\n",
            " [ 67  62]\n",
            " [100 100]\n",
            " [ 54  51]\n",
            " [ 71  63]\n",
            " [ 98 100]\n",
            " [ 84  81]\n",
            " [ 65  61]\n",
            " [ 43  42]\n",
            " [ 45  47]\n",
            " [ 57  52]\n",
            " [ 80  84]\n",
            " [ 65  73]\n",
            " [ 63  62]\n",
            " [ 65  67]\n",
            " [ 87  89]\n",
            " [ 84  76]\n",
            " [ 74  70]\n",
            " [ 54  55]\n",
            " [ 49  44]\n",
            " [ 70  73]\n",
            " [ 95  99]\n",
            " [ 90  83]\n",
            " [ 48  48]\n",
            " [ 68  72]\n",
            " [ 33  31]\n",
            " [ 86  81]\n",
            " [ 56  53]\n",
            " [ 69  74]\n",
            " [ 65  61]\n",
            " [ 71  59]\n",
            " [ 65  63]\n",
            " [ 60  52]\n",
            " [ 64  69]\n",
            " [ 66  63]\n",
            " [ 65  59]\n",
            " [ 81  85]\n",
            " [ 88  94]\n",
            " [ 71  77]\n",
            " [ 48  43]\n",
            " [ 85  77]\n",
            " [ 59  59]\n",
            " [ 55  58]\n",
            " [ 86  93]\n",
            " [ 65  75]\n",
            " [ 76  72]\n",
            " [ 81  82]\n",
            " [ 91  94]\n",
            " [ 46  48]\n",
            " [ 57  56]\n",
            " [ 78  77]\n",
            " [ 81  77]\n",
            " [ 77  75]\n",
            " [ 57  56]\n",
            " [ 82  82]\n",
            " [ 56  64]\n",
            " [ 70  71]\n",
            " [ 92  87]\n",
            " [ 48  39]\n",
            " [ 41  45]\n",
            " [ 87  86]\n",
            " [ 63  66]\n",
            " [ 70  69]\n",
            " [ 72  85]\n",
            " [ 84  80]\n",
            " [ 49  50]\n",
            " [ 72  78]\n",
            " [ 77  72]\n",
            " [ 46  41]\n",
            " [ 80  79]\n",
            " [ 34  34]\n",
            " [ 61  62]\n",
            " [ 72  75]\n",
            " [ 74  71]\n",
            " [ 55  58]\n",
            " [ 59  62]\n",
            " [ 62  62]\n",
            " [ 75  76]\n",
            " [ 56  63]\n",
            " [ 68  67]\n",
            " [ 65  74]\n",
            " [ 74  84]\n",
            " [ 75  80]\n",
            " [ 76  74]]\n",
            "\n",
            "Weight Vector W:\n",
            "[[0.70946747]\n",
            " [0.6793671 ]]\n",
            "\n",
            "Predicted Y values (Training):\n",
            "[[111.16696649]\n",
            " [ 95.18031873]\n",
            " [ 31.88299441]\n",
            " [ 97.89778712]\n",
            " [120.11914027]\n",
            " [114.56380199]\n",
            " [104.78175924]\n",
            " [ 91.72328248]\n",
            " [108.41939772]\n",
            " [ 94.531052  ]\n",
            " [123.01721091]\n",
            " [135.45652131]\n",
            " [ 98.75775646]\n",
            " [104.96236147]\n",
            " [101.47522486]\n",
            " [101.29462262]\n",
            " [ 98.04828899]\n",
            " [ 61.78808826]\n",
            " [ 62.61795722]\n",
            " [107.6497295 ]\n",
            " [ 73.69853342]\n",
            " [117.40167187]\n",
            " [ 87.55677877]\n",
            " [107.52932801]\n",
            " [ 97.80748601]\n",
            " [109.74803154]\n",
            " [106.94026203]\n",
            " [111.10676574]\n",
            " [134.03758636]\n",
            " [119.53007429]\n",
            " [ 82.68080758]\n",
            " [ 74.93686613]\n",
            " [ 76.50630294]\n",
            " [ 90.36454828]\n",
            " [ 93.11211705]\n",
            " [120.23954176]\n",
            " [ 86.78711055]\n",
            " [ 88.26624624]\n",
            " [ 82.03154085]\n",
            " [ 77.09536892]\n",
            " [112.55580106]\n",
            " [118.76040607]\n",
            " [108.29899623]\n",
            " [ 87.49657802]\n",
            " [ 96.44875181]\n",
            " [ 61.90848975]\n",
            " [ 99.99608917]\n",
            " [ 65.95459197]\n",
            " [ 63.2070232 ]\n",
            " [104.13249251]\n",
            " [100.67545627]\n",
            " [ 72.15919699]\n",
            " [ 77.80483639]\n",
            " [ 63.23712357]\n",
            " [118.82060682]\n",
            " [ 89.71528156]\n",
            " [ 68.17329551]\n",
            " [ 85.45847672]\n",
            " [123.4858754 ]\n",
            " [117.34147113]\n",
            " [ 72.98906595]\n",
            " [122.15724157]\n",
            " [ 90.21404642]\n",
            " [107.70993024]\n",
            " [115.3334702 ]\n",
            " [ 79.87303806]\n",
            " [138.88345718]\n",
            " [110.5779005 ]\n",
            " [115.27326946]\n",
            " [112.43539957]\n",
            " [118.60990421]\n",
            " [118.76040607]\n",
            " [100.05628991]\n",
            " [125.79488005]\n",
            " [110.95626388]\n",
            " [107.6497295 ]\n",
            " [117.43177225]\n",
            " [131.22981685]\n",
            " [111.13686611]\n",
            " [103.45312541]\n",
            " [104.87206036]\n",
            " [ 68.11309476]\n",
            " [108.38929734]\n",
            " [ 93.70118304]\n",
            " [ 81.94123973]\n",
            " [ 97.21842002]\n",
            " [109.71793117]\n",
            " [ 82.62060683]\n",
            " [ 97.98808824]\n",
            " [ 62.43735499]\n",
            " [ 85.36817561]\n",
            " [ 94.50095163]\n",
            " [ 56.82181595]\n",
            " [ 92.28224809]\n",
            " [ 74.99706688]\n",
            " [ 84.83931037]\n",
            " [ 96.01018769]\n",
            " [114.62400273]\n",
            " [ 61.90848975]\n",
            " [ 65.98469235]\n",
            " [ 54.28494979]\n",
            " [120.17934102]\n",
            " [ 98.6072546 ]\n",
            " [102.86405943]\n",
            " [110.3671979 ]\n",
            " [116.07303805]\n",
            " [131.96938469]\n",
            " [ 93.76138378]\n",
            " [ 68.67206037]\n",
            " [120.70820626]\n",
            " [ 86.16794419]\n",
            " [ 66.63395907]\n",
            " [ 82.00144048]\n",
            " [ 82.00144048]\n",
            " [ 97.12811891]\n",
            " [ 79.93323881]\n",
            " [ 90.95361427]\n",
            " [133.35821926]\n",
            " [106.26089493]\n",
            " [105.52132708]\n",
            " [ 88.91551297]\n",
            " [108.29899623]\n",
            " [ 90.89341352]\n",
            " [113.23516816]\n",
            " [ 86.75701018]\n",
            " [106.75965979]\n",
            " [128.4521477 ]\n",
            " [106.9703624 ]\n",
            " [ 53.60558269]\n",
            " [ 75.73663472]\n",
            " [128.4521477 ]\n",
            " [129.1315148 ]\n",
            " [ 86.07764308]\n",
            " [117.43177225]\n",
            " [ 91.04391538]\n",
            " [ 93.26261892]\n",
            " [120.82860774]\n",
            " [135.45652131]\n",
            " [115.21306871]\n",
            " [117.40167187]\n",
            " [ 77.65433453]\n",
            " [ 92.34244884]\n",
            " [ 86.84731129]\n",
            " [ 63.94659105]\n",
            " [ 70.27159755]\n",
            " [ 88.94561334]\n",
            " [127.80288098]\n",
            " [ 67.43372766]\n",
            " [ 72.18929736]\n",
            " [116.66210403]\n",
            " [ 78.51430386]\n",
            " [104.99246184]\n",
            " [ 87.55677877]\n",
            " [ 80.55240516]\n",
            " [107.61962913]\n",
            " [110.45749901]\n",
            " [134.62665235]\n",
            " [ 89.53467932]\n",
            " [117.28127038]\n",
            " [ 85.57887821]\n",
            " [ 61.19902228]\n",
            " [118.85070719]\n",
            " [107.70993024]\n",
            " [ 66.78446093]\n",
            " [ 91.10411613]\n",
            " [107.6497295 ]\n",
            " [ 93.11211705]\n",
            " [ 95.79948508]\n",
            " [ 95.85968582]\n",
            " [112.52570069]\n",
            " [129.90118302]\n",
            " [ 83.39027505]\n",
            " [ 86.90751204]\n",
            " [ 82.65070721]\n",
            " [115.98273693]\n",
            " [133.35821926]\n",
            " [ 86.78711055]\n",
            " [ 86.81721092]\n",
            " [ 95.18031873]\n",
            " [ 77.86503714]\n",
            " [ 99.2866217 ]\n",
            " [120.0890399 ]\n",
            " [119.43977317]\n",
            " [109.77813191]\n",
            " [ 68.85266261]\n",
            " [107.00046277]\n",
            " [ 93.82158453]\n",
            " [ 93.76138378]\n",
            " [ 82.62060683]\n",
            " [ 82.68080758]\n",
            " [117.31137076]\n",
            " [102.18469233]\n",
            " [ 88.91551297]\n",
            " [106.9703624 ]\n",
            " [ 54.25484942]\n",
            " [ 71.63033175]\n",
            " [111.81623321]\n",
            " [ 73.57813193]\n",
            " [ 95.15021835]\n",
            " [125.58417744]\n",
            " [118.08103897]\n",
            " [ 99.43712356]\n",
            " [108.44949809]\n",
            " [ 79.99343955]\n",
            " [ 99.34682244]\n",
            " [ 56.94221744]\n",
            " [ 84.65870813]\n",
            " [127.0934135 ]\n",
            " [122.24754269]\n",
            " [124.28564399]\n",
            " [ 81.29197301]\n",
            " [ 92.31234846]\n",
            " [122.27764306]\n",
            " [ 89.56477969]\n",
            " [ 84.68880851]\n",
            " [ 89.56477969]\n",
            " [ 73.6082323 ]\n",
            " [ 76.44610219]\n",
            " [111.87643396]\n",
            " [ 80.55240516]\n",
            " [121.59827596]\n",
            " [ 98.66745534]\n",
            " [ 92.46285033]\n",
            " [ 90.89341352]\n",
            " [109.71793117]\n",
            " [102.77375831]\n",
            " [108.29899623]\n",
            " [127.0934135 ]\n",
            " [120.768407  ]\n",
            " [ 92.37254921]\n",
            " [126.26354454]\n",
            " [117.3715715 ]\n",
            " [126.35384566]\n",
            " [ 61.1689219 ]\n",
            " [ 91.69318211]\n",
            " [ 61.78808826]\n",
            " [ 95.70918396]\n",
            " [ 87.61697951]\n",
            " [ 60.39925369]\n",
            " [ 91.10411613]\n",
            " [114.4434005 ]\n",
            " [ 97.06791816]\n",
            " [ 96.53905292]\n",
            " [ 83.94924066]\n",
            " [127.80288098]\n",
            " [ 94.41065051]\n",
            " [128.60264956]\n",
            " [113.2953689 ]\n",
            " [ 71.44972952]\n",
            " [111.9366347 ]\n",
            " [ 99.25652132]\n",
            " [107.52932801]\n",
            " [120.70820626]\n",
            " [ 98.78785683]\n",
            " [ 88.85531222]\n",
            " [115.3334702 ]\n",
            " [ 89.59488007]\n",
            " [ 89.56477969]\n",
            " [ 88.14584475]\n",
            " [ 85.54877784]\n",
            " [ 92.31234846]\n",
            " [122.98711053]\n",
            " [137.46452223]\n",
            " [113.91453526]\n",
            " [ 88.79511148]\n",
            " [ 55.5232825 ]\n",
            " [ 71.41962914]\n",
            " [113.82423414]\n",
            " [ 78.54440424]\n",
            " [115.42377132]\n",
            " [ 84.06964215]\n",
            " [104.75165887]\n",
            " [131.22981685]\n",
            " [117.96063748]\n",
            " [102.00409009]\n",
            " [ 86.84731129]\n",
            " [ 84.80921   ]\n",
            " [131.22981685]\n",
            " [ 99.31672207]\n",
            " [ 96.38855106]\n",
            " [131.22981685]\n",
            " [ 89.62498044]\n",
            " [123.57617652]\n",
            " [ 98.72765609]\n",
            " [ 34.81116541]\n",
            " [ 70.77036242]\n",
            " [ 85.42837635]\n",
            " [105.64172857]\n",
            " [102.09439121]\n",
            " [116.81260589]\n",
            " [138.88345718]\n",
            " [120.23954176]\n",
            " [104.81185961]\n",
            " [ 39.65703623]\n",
            " [ 72.2494981 ]\n",
            " [104.9322611 ]\n",
            " [ 99.37692281]\n",
            " [ 50.14854645]\n",
            " [ 80.58250554]\n",
            " [ 77.6844349 ]\n",
            " [111.81623321]\n",
            " [ 88.14584475]\n",
            " [ 79.84293769]\n",
            " [ 76.47620256]\n",
            " [ 93.76138378]\n",
            " [104.90216073]\n",
            " [138.88345718]\n",
            " [114.56380199]\n",
            " [ 90.24414679]\n",
            " [102.03419047]\n",
            " [101.47522486]\n",
            " [ 62.55775647]\n",
            " [ 95.76938471]\n",
            " [ 62.46745536]\n",
            " [ 84.80921   ]\n",
            " [112.52570069]\n",
            " [ 84.77910962]\n",
            " [ 88.23614587]\n",
            " [ 77.89513751]\n",
            " [102.03419047]\n",
            " [101.41502411]\n",
            " [100.70555664]\n",
            " [115.18296834]\n",
            " [113.14486704]\n",
            " [ 53.4851812 ]\n",
            " [ 76.47620256]\n",
            " [ 87.67718026]\n",
            " [ 95.76938471]\n",
            " [ 77.06526855]\n",
            " [101.97398972]\n",
            " [ 86.13784382]\n",
            " [ 84.00944141]\n",
            " [110.42739864]\n",
            " [104.84195998]\n",
            " [ 68.76236149]\n",
            " [ 77.80483639]\n",
            " [ 80.52230479]\n",
            " [ 77.6844349 ]\n",
            " [107.00046277]\n",
            " [110.45749901]\n",
            " [113.85433451]\n",
            " [ 61.22912265]\n",
            " [114.56380199]\n",
            " [110.42739864]\n",
            " [104.19269326]\n",
            " [ 91.66308174]\n",
            " [ 78.48420349]\n",
            " [ 91.69318211]\n",
            " [138.88345718]\n",
            " [ 55.5232825 ]\n",
            " [ 77.77473602]\n",
            " [ 86.72690981]\n",
            " [ 79.93323881]\n",
            " [ 64.68615889]\n",
            " [ 94.44075088]\n",
            " [120.82860774]\n",
            " [ 91.75338285]\n",
            " [ 91.16431687]\n",
            " [ 71.570131  ]\n",
            " [102.18469233]\n",
            " [107.55942838]\n",
            " [ 95.73928434]\n",
            " [111.87643396]\n",
            " [110.33709753]\n",
            " [ 25.02912266]\n",
            " [130.58055012]\n",
            " [ 96.32835032]\n",
            " [ 97.15821928]\n",
            " [ 82.65070721]\n",
            " [ 91.01381501]\n",
            " [ 81.35217375]\n",
            " [ 70.77036242]\n",
            " [ 62.58785685]\n",
            " [138.88345718]\n",
            " [ 88.17594512]\n",
            " [121.4477741 ]\n",
            " [101.324723  ]\n",
            " [ 89.62498044]\n",
            " [ 91.01381501]\n",
            " [112.43539957]\n",
            " [ 74.31769978]\n",
            " [102.12449158]\n",
            " [ 64.62595815]\n",
            " [ 90.95361427]\n",
            " [ 86.84731129]\n",
            " [ 61.19902228]\n",
            " [ 70.98106502]\n",
            " [ 97.2485204 ]\n",
            " [ 98.69755571]\n",
            " [124.3759451 ]\n",
            " [108.44949809]\n",
            " [ 88.11574438]\n",
            " [126.4140464 ]\n",
            " [115.89243581]\n",
            " [ 73.63833268]\n",
            " [ 79.96333918]\n",
            " [ 63.82618956]\n",
            " [ 95.8897862 ]\n",
            " [136.16598878]\n",
            " [137.52472298]\n",
            " [ 63.94659105]\n",
            " [ 79.81283732]\n",
            " [100.61525552]\n",
            " [108.38929734]\n",
            " [109.0084637 ]\n",
            " [101.29462262]\n",
            " [ 81.91113936]\n",
            " [113.94463563]\n",
            " [113.88443489]\n",
            " [ 98.51695348]\n",
            " [ 93.70118304]\n",
            " [121.62837633]\n",
            " [ 88.085644  ]\n",
            " [122.24754269]\n",
            " [117.99073786]\n",
            " [ 94.47085125]\n",
            " [113.23516816]\n",
            " [ 65.27522487]\n",
            " [ 77.92523788]\n",
            " [102.03419047]\n",
            " [108.29899623]\n",
            " [102.77375831]\n",
            " [ 82.62060683]\n",
            " [102.83395906]\n",
            " [102.00409009]\n",
            " [ 77.74463565]\n",
            " [138.88345718]\n",
            " [ 98.63735497]\n",
            " [111.84633359]\n",
            " [ 62.40725461]\n",
            " [ 88.26624624]\n",
            " [106.38129642]\n",
            " [ 90.92351389]\n",
            " [124.90481034]\n",
            " [133.23781777]\n",
            " [109.71793117]\n",
            " [ 93.02181594]\n",
            " [113.32546928]\n",
            " [133.35821926]\n",
            " [ 77.80483639]\n",
            " [ 77.83493676]\n",
            " [ 93.94198602]\n",
            " [108.29899623]\n",
            " [ 90.33444791]\n",
            " [ 93.82158453]\n",
            " [118.17134009]\n",
            " [100.67545627]\n",
            " [ 51.47718027]\n",
            " [ 55.58348324]\n",
            " [104.16259288]\n",
            " [ 76.38590145]\n",
            " [107.06066352]\n",
            " [ 94.47085125]\n",
            " [ 88.26624624]\n",
            " [128.48224808]\n",
            " [138.20409008]\n",
            " [ 90.33444791]\n",
            " [ 88.94561334]\n",
            " [ 57.59148417]\n",
            " [130.52034937]\n",
            " [100.6453559 ]\n",
            " [ 84.71890888]\n",
            " [ 90.92351389]\n",
            " [ 99.34682244]\n",
            " [110.48759939]\n",
            " [ 93.08201668]\n",
            " [ 65.95459197]\n",
            " [ 78.45410312]\n",
            " [ 85.51867747]\n",
            " [100.02618954]\n",
            " [127.12351388]\n",
            " [ 99.9659888 ]\n",
            " [ 63.2070232 ]\n",
            " [ 53.4851812 ]\n",
            " [107.55942838]\n",
            " [ 68.79246186]\n",
            " [116.6922044 ]\n",
            " [ 84.1298429 ]\n",
            " [120.88880849]\n",
            " [ 66.72426019]\n",
            " [ 87.43637728]\n",
            " [ 93.26261892]\n",
            " [ 80.49220442]\n",
            " [109.6878308 ]\n",
            " [ 76.50630294]\n",
            " [ 71.570131  ]\n",
            " [ 97.95798787]\n",
            " [ 97.21842002]\n",
            " [116.0128373 ]\n",
            " [ 65.2451245 ]\n",
            " [114.53370161]\n",
            " [ 95.05991724]\n",
            " [ 73.6082323 ]\n",
            " [ 54.96431689]\n",
            " [114.56380199]\n",
            " [ 66.69415982]\n",
            " [107.00046277]\n",
            " [ 58.33105201]\n",
            " [ 83.39027505]\n",
            " [ 84.71890888]\n",
            " [101.41502411]\n",
            " [ 75.6463336 ]\n",
            " [121.59827596]\n",
            " [101.38492374]\n",
            " [ 86.25824531]\n",
            " [ 68.79246186]\n",
            " [138.88345718]\n",
            " [ 90.98371464]\n",
            " [109.65773043]\n",
            " [115.89243581]\n",
            " [ 68.88276298]\n",
            " [107.74003062]\n",
            " [ 88.88541259]\n",
            " [ 59.16092098]\n",
            " [102.15459196]\n",
            " [108.3290966 ]\n",
            " [ 84.15994327]\n",
            " [123.72667838]\n",
            " [ 92.34244884]\n",
            " [ 90.39464865]\n",
            " [ 76.41600182]\n",
            " [ 89.00581408]\n",
            " [102.62325645]\n",
            " [ 95.79948508]\n",
            " [ 93.79148415]\n",
            " [138.88345718]\n",
            " [118.02083823]\n",
            " [ 87.49657802]\n",
            " [ 91.60288099]\n",
            " [ 93.82158453]\n",
            " [ 69.38152784]\n",
            " [ 74.34780015]\n",
            " [ 47.31067656]\n",
            " [108.3290966 ]\n",
            " [ 60.39925369]\n",
            " [ 79.19367096]\n",
            " [101.41502411]\n",
            " [113.91453526]\n",
            " [ 98.63735497]\n",
            " [105.52132708]\n",
            " [116.04293767]\n",
            " [ 79.96333918]\n",
            " [121.59827596]\n",
            " [106.2909953 ]\n",
            " [113.94463563]\n",
            " [ 86.10774345]\n",
            " [ 55.49318213]\n",
            " [ 89.65508081]\n",
            " [ 73.07936707]\n",
            " [ 72.9288652 ]\n",
            " [ 90.4548494 ]\n",
            " [ 84.00944141]\n",
            " [ 88.79511148]\n",
            " [102.74365794]\n",
            " [ 99.4973243 ]\n",
            " [101.41502411]\n",
            " [ 95.2104191 ]\n",
            " [ 73.69853342]\n",
            " [117.31137076]\n",
            " [ 97.15821928]\n",
            " [121.47787447]\n",
            " [ 58.39125276]\n",
            " [102.83395906]\n",
            " [102.80385868]\n",
            " [ 86.16794419]\n",
            " [135.48662168]\n",
            " [103.54342653]\n",
            " [ 76.35580108]\n",
            " [ 95.79948508]\n",
            " [103.51332616]\n",
            " [ 84.68880851]\n",
            " [ 79.93323881]\n",
            " [104.9322611 ]\n",
            " [ 94.50095163]\n",
            " [123.69657801]\n",
            " [111.16696649]\n",
            " [137.46452223]\n",
            " [ 81.29197301]\n",
            " [106.17059381]\n",
            " [ 86.13784382]\n",
            " [111.10676574]\n",
            " [118.0509386 ]\n",
            " [ 69.47182896]\n",
            " [ 82.80120907]\n",
            " [102.68345719]\n",
            " [114.59390236]\n",
            " [125.08541258]\n",
            " [ 82.59050646]\n",
            " [111.16696649]\n",
            " [102.00409009]\n",
            " [ 88.94561334]\n",
            " [ 84.77910962]\n",
            " [ 88.20604549]\n",
            " [ 62.55775647]\n",
            " [137.46452223]\n",
            " [ 97.9278875 ]\n",
            " [ 84.80921   ]\n",
            " [ 79.87303806]\n",
            " [ 96.53905292]\n",
            " [104.78175924]\n",
            " [104.90216073]\n",
            " [ 93.73128341]\n",
            " [ 75.73663472]\n",
            " [115.92253618]\n",
            " [ 90.92351389]\n",
            " [101.35482337]\n",
            " [ 88.91551297]\n",
            " [102.06429084]\n",
            " [ 68.17329551]\n",
            " [ 88.11574438]\n",
            " [ 85.33807523]\n",
            " [ 84.65870813]\n",
            " [131.35021834]\n",
            " [ 81.91113936]\n",
            " [107.03056314]\n",
            " [ 76.29560033]\n",
            " [ 97.2485204 ]\n",
            " [120.26964213]\n",
            " [ 99.25652132]\n",
            " [ 56.23274997]\n",
            " [104.84195998]\n",
            " [114.62400273]\n",
            " [102.15459196]\n",
            " [138.88345718]\n",
            " [ 89.53467932]\n",
            " [ 93.79148415]\n",
            " [ 59.8101877 ]\n",
            " [ 81.26187264]\n",
            " [ 59.78008733]\n",
            " [110.45749901]\n",
            " [101.47522486]\n",
            " [ 95.79948508]\n",
            " [106.2909953 ]\n",
            " [128.51234845]\n",
            " [ 87.52667839]\n",
            " [ 66.10509383]\n",
            " [133.35821926]\n",
            " [ 91.69318211]\n",
            " [ 99.37692281]\n",
            " [110.33709753]\n",
            " [104.19269326]\n",
            " [ 79.96333918]\n",
            " [ 97.30872114]\n",
            " [138.88345718]\n",
            " [109.09876482]\n",
            " [ 88.91551297]\n",
            " [100.70555664]\n",
            " [103.48322578]\n",
            " [ 81.94123973]\n",
            " [104.81185961]\n",
            " [101.41502411]\n",
            " [ 88.44684847]\n",
            " [120.11914027]\n",
            " [126.29364491]\n",
            " [ 77.06526855]\n",
            " [ 98.04828899]\n",
            " [ 83.42037542]\n",
            " [ 83.85893954]\n",
            " [ 97.9278875 ]\n",
            " [100.73565701]\n",
            " [114.59390236]\n",
            " [ 81.91113936]\n",
            " [134.00748599]\n",
            " [113.11476667]\n",
            " [101.47522486]\n",
            " [ 70.15119606]\n",
            " [ 71.50993026]\n",
            " [131.10941536]\n",
            " [132.02958544]\n",
            " [107.55942838]\n",
            " [107.03056314]\n",
            " [ 95.76938471]\n",
            " [111.81623321]\n",
            " [108.44949809]\n",
            " [ 88.91551297]\n",
            " [ 98.57715422]\n",
            " [ 59.04051949]\n",
            " [ 98.78785683]\n",
            " [ 97.21842002]\n",
            " [119.56017466]\n",
            " [120.14924064]\n",
            " [ 80.55240516]\n",
            " [117.34147113]\n",
            " [119.46987355]\n",
            " [108.29899623]\n",
            " [ 65.2451245 ]\n",
            " [ 97.95798787]\n",
            " [110.45749901]\n",
            " [125.64437819]\n",
            " [ 83.36017468]\n",
            " [ 75.05726762]\n",
            " [ 89.62498044]\n",
            " [112.64610218]\n",
            " [ 73.63833268]\n",
            " [ 69.47182896]\n",
            " [ 86.78711055]\n",
            " [127.86308172]\n",
            " [ 72.2494981 ]\n",
            " [120.0890399 ]\n",
            " [122.15724157]\n",
            " [121.4477741 ]\n",
            " [ 86.84731129]\n",
            " [ 86.13784382]\n",
            " [101.324723  ]\n",
            " [ 89.03591446]\n",
            " [ 92.22204735]\n",
            " [104.75165887]\n",
            " [ 79.40437357]\n",
            " [ 61.75798789]\n",
            " [ 86.16794419]\n",
            " [100.76575738]\n",
            " [128.48224808]\n",
            " [109.03856407]\n",
            " [118.02083823]\n",
            " [116.66210403]\n",
            " [106.38129642]\n",
            " [ 99.34682244]\n",
            " [102.71355757]\n",
            " [113.0846663 ]\n",
            " [112.52570069]\n",
            " [ 66.01479272]\n",
            " [ 60.33905294]\n",
            " [ 84.15994327]\n",
            " [107.67982987]\n",
            " [ 52.86601484]\n",
            " [ 72.7181626 ]\n",
            " [ 67.34342654]\n",
            " [ 97.98808824]\n",
            " [ 78.45410312]\n",
            " [ 96.5691533 ]\n",
            " [110.3671979 ]\n",
            " [108.29899623]\n",
            " [ 98.01818861]\n",
            " [ 49.90774347]\n",
            " [107.6497295 ]\n",
            " [107.55942838]\n",
            " [100.73565701]\n",
            " [ 75.76673509]\n",
            " [ 60.42935406]\n",
            " [118.08103897]\n",
            " [117.31137076]\n",
            " [129.99148414]\n",
            " [ 86.22814494]\n",
            " [ 95.18031873]\n",
            " [111.75603247]\n",
            " [116.6922044 ]\n",
            " [ 73.07936707]\n",
            " [ 85.39827598]\n",
            " [ 63.26722395]\n",
            " [ 76.44610219]\n",
            " [ 98.66745534]\n",
            " [127.03321276]\n",
            " [ 67.49392841]\n",
            " [114.6541031 ]\n",
            " [ 78.60460498]\n",
            " [109.77813191]\n",
            " [114.6541031 ]\n",
            " [115.98273693]\n",
            " [ 95.12011798]\n",
            " [ 94.47085125]\n",
            " [103.48322578]\n",
            " [102.00409009]\n",
            " [ 95.82958545]\n",
            " [ 95.76938471]\n",
            " [ 72.2494981 ]\n",
            " [ 91.66308174]\n",
            " [116.72230477]\n",
            " [ 92.31234846]\n",
            " [115.95263656]\n",
            " [ 79.22377134]\n",
            " [ 80.67280665]\n",
            " [124.28564399]\n",
            " [ 90.95361427]\n",
            " [ 72.21939773]\n",
            " [111.10676574]\n",
            " [102.09439121]\n",
            " [ 93.8516849 ]\n",
            " [ 22.99102137]\n",
            " [105.67182894]\n",
            " [ 90.27424717]\n",
            " [104.19269326]\n",
            " [ 97.21842002]\n",
            " [107.55942838]\n",
            " [116.10313842]\n",
            " [ 97.86768675]\n",
            " [108.35919697]\n",
            " [110.27689678]\n",
            " [112.58590143]\n",
            " [ 93.73128341]\n",
            " [ 99.9659888 ]\n",
            " [107.03056314]\n",
            " [ 68.79246186]\n",
            " [ 77.15556966]\n",
            " [ 66.72426019]\n",
            " [ 79.22377134]\n",
            " [ 99.40702319]\n",
            " [131.29001759]\n",
            " [107.58952875]\n",
            " [104.16259288]\n",
            " [108.26889585]]\n",
            "\n",
            "Predicted Y values (Testing):\n",
            "[[ 95.82958545]\n",
            " [ 54.10434755]\n",
            " [ 82.71090795]\n",
            " [ 79.90313844]\n",
            " [125.0553122 ]\n",
            " [ 97.21842002]\n",
            " [138.17398971]\n",
            " [ 62.64805759]\n",
            " [ 81.97134011]\n",
            " [111.75603247]\n",
            " [116.75240515]\n",
            " [ 89.65508081]\n",
            " [ 91.75338285]\n",
            " [ 77.15556966]\n",
            " [ 81.94123973]\n",
            " [ 73.69853342]\n",
            " [ 94.531052  ]\n",
            " [ 91.07401575]\n",
            " [123.60627689]\n",
            " [ 88.23614587]\n",
            " [111.16696649]\n",
            " [100.05628991]\n",
            " [ 88.94561334]\n",
            " [102.77375831]\n",
            " [ 79.16357059]\n",
            " [119.49997392]\n",
            " [ 72.2494981 ]\n",
            " [ 85.54877784]\n",
            " [116.0128373 ]\n",
            " [ 65.18492376]\n",
            " [134.03758636]\n",
            " [111.22716723]\n",
            " [ 44.4427063 ]\n",
            " [ 77.06526855]\n",
            " [118.11113935]\n",
            " [106.91016165]\n",
            " [128.36184659]\n",
            " [122.1271412 ]\n",
            " [ 99.40702319]\n",
            " [110.45749901]\n",
            " [113.02446555]\n",
            " [ 84.74900925]\n",
            " [ 88.79511148]\n",
            " [101.35482337]\n",
            " [ 80.58250554]\n",
            " [115.18296834]\n",
            " [111.90653433]\n",
            " [112.6160018 ]\n",
            " [ 54.1645483 ]\n",
            " [110.42739864]\n",
            " [ 70.92086428]\n",
            " [100.70555664]\n",
            " [ 87.52667839]\n",
            " [ 99.22642095]\n",
            " [ 70.18129643]\n",
            " [104.90216073]\n",
            " [ 52.03614588]\n",
            " [ 92.43274995]\n",
            " [103.45312541]\n",
            " [120.20944139]\n",
            " [ 66.72426019]\n",
            " [ 69.44172859]\n",
            " [ 88.79511148]\n",
            " [ 86.22814494]\n",
            " [ 80.02353992]\n",
            " [124.96501109]\n",
            " [138.88345718]\n",
            " [ 93.82158453]\n",
            " [ 84.03954178]\n",
            " [120.11914027]\n",
            " [ 64.50555666]\n",
            " [138.17398971]\n",
            " [ 86.10774345]\n",
            " [117.25117001]\n",
            " [ 88.94561334]\n",
            " [ 81.4123745 ]\n",
            " [111.16696649]\n",
            " [ 62.46745536]\n",
            " [ 96.47885218]\n",
            " [ 69.29122673]\n",
            " [103.51332616]\n",
            " [ 97.83758638]\n",
            " [ 86.04754271]\n",
            " [115.27326946]\n",
            " [ 99.22642095]\n",
            " [114.59390236]\n",
            " [111.16696649]\n",
            " [ 86.07764308]\n",
            " [ 88.91551297]\n",
            " [102.06429084]\n",
            " [ 98.6072546 ]\n",
            " [ 92.34244884]\n",
            " [ 81.23177226]\n",
            " [ 82.71090795]\n",
            " [ 36.9696682 ]\n",
            " [122.86670904]\n",
            " [110.39729827]\n",
            " [111.07666537]\n",
            " [111.87643396]\n",
            " [ 96.59925367]\n",
            " [105.6116282 ]\n",
            " [ 44.41260592]\n",
            " [116.57180291]\n",
            " [ 47.43107805]\n",
            " [133.35821926]\n",
            " [111.046565  ]\n",
            " [ 94.47085125]\n",
            " [102.2147927 ]\n",
            " [ 99.25652132]\n",
            " [ 82.71090795]\n",
            " [100.61525552]\n",
            " [132.02958544]\n",
            " [103.48322578]\n",
            " [107.03056314]\n",
            " [122.18734194]\n",
            " [106.78976017]\n",
            " [ 95.18031873]\n",
            " [ 89.65508081]\n",
            " [138.88345718]\n",
            " [ 72.95896558]\n",
            " [ 93.1723178 ]\n",
            " [137.46452223]\n",
            " [114.62400273]\n",
            " [ 87.55677877]\n",
            " [ 59.04051949]\n",
            " [ 63.85628993]\n",
            " [ 75.76673509]\n",
            " [113.82423414]\n",
            " [ 95.70918396]\n",
            " [ 86.81721092]\n",
            " [ 91.63298136]\n",
            " [122.18734194]\n",
            " [111.22716723]\n",
            " [100.05628991]\n",
            " [ 75.67643398]\n",
            " [ 64.65605852]\n",
            " [ 99.25652132]\n",
            " [134.65675272]\n",
            " [120.23954176]\n",
            " [ 66.66405945]\n",
            " [ 97.15821928]\n",
            " [ 44.47280667]\n",
            " [116.04293767]\n",
            " [ 75.73663472]\n",
            " [ 99.22642095]\n",
            " [ 87.55677877]\n",
            " [ 90.4548494 ]\n",
            " [ 88.91551297]\n",
            " [ 77.89513751]\n",
            " [ 92.28224809]\n",
            " [ 89.62498044]\n",
            " [ 86.19804457]\n",
            " [115.21306871]\n",
            " [126.29364491]\n",
            " [102.68345719]\n",
            " [ 63.26722395]\n",
            " [112.6160018 ]\n",
            " [ 81.94123973]\n",
            " [ 78.42400275]\n",
            " [124.19534287]\n",
            " [ 97.06791816]\n",
            " [102.83395906]\n",
            " [113.17496741]\n",
            " [128.42204733]\n",
            " [ 65.2451245 ]\n",
            " [ 78.48420349]\n",
            " [107.6497295 ]\n",
            " [109.77813191]\n",
            " [105.58152783]\n",
            " [ 78.48420349]\n",
            " [113.88443489]\n",
            " [ 83.20967282]\n",
            " [ 97.89778712]\n",
            " [124.3759451 ]\n",
            " [ 60.54975555]\n",
            " [ 59.65968584]\n",
            " [120.14924064]\n",
            " [ 89.53467932]\n",
            " [ 96.53905292]\n",
            " [108.82786146]\n",
            " [113.94463563]\n",
            " [ 68.73226112]\n",
            " [104.07229177]\n",
            " [103.54342653]\n",
            " [ 60.4895548 ]\n",
            " [110.42739864]\n",
            " [ 47.22037544]\n",
            " [ 85.39827598]\n",
            " [102.03419047]\n",
            " [100.73565701]\n",
            " [ 78.42400275]\n",
            " [ 83.97934103]\n",
            " [ 86.10774345]\n",
            " [104.84195998]\n",
            " [ 82.53030572]\n",
            " [ 93.76138378]\n",
            " [ 96.38855106]\n",
            " [109.56742931]\n",
            " [107.55942838]\n",
            " [104.19269326]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent from Scratch:"
      ],
      "metadata": {
        "id": "Hwbf685i-Rzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Example dataset\n",
        "data = {\n",
        "    'Math': [48, 62, 79, 76, 59],\n",
        "    'Reading': [68, 81, 80, 83, 64],\n",
        "    'Writing': [63, 72, 78, 79, 62]\n",
        "}\n",
        "\n",
        "# Convert the dataset to a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Prepare the feature matrix (X) and target vector (Y)\n",
        "X = df[['Math', 'Reading']].values  # Features: Math and Reading scores\n",
        "Y = df[['Writing']].values          # Target: Writing scores\n",
        "\n",
        "# Add a column of ones for the bias term in X\n",
        "X = np.c_[np.ones(X.shape[0]), X]    # Adding bias (intercept term)\n",
        "\n",
        "# Initialize weights (W)\n",
        "W = np.zeros((X.shape[1], 1))  # Initializing weights to zero\n",
        "\n",
        "# Gradient Descent Parameters\n",
        "alpha = 0.01  # Learning rate\n",
        "iterations = 1000  # Number of iterations\n",
        "\n",
        "# Gradient Descent Function\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix (m x n).\n",
        "    Y (numpy.ndarray): Target vector (m x 1).\n",
        "    W (numpy.ndarray): Initial guess for parameters (n x 1).\n",
        "    alpha (float): Learning rate.\n",
        "    iterations (int): Number of iterations for gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the final optimized parameters (W_update) and the history of cost values.\n",
        "    W_update (numpy.ndarray): Updated parameters (n x 1).\n",
        "    cost_history (list): History of cost values over iterations.\n",
        "    \"\"\"\n",
        "    # Initialize cost history\n",
        "    cost_history = [0] * iterations\n",
        "\n",
        "    # Number of samples\n",
        "    m = len(Y)\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        # Step 1: Hypothesis Values\n",
        "        Y_pred = np.dot(X, W)  # Linear hypothesis: X * W\n",
        "\n",
        "        # Step 2: Difference between Hypothesis and Actual Y\n",
        "        loss = Y_pred - Y  # Residuals: predicted Y minus actual Y\n",
        "\n",
        "        # Step 3: Gradient Calculation\n",
        "        dw = (1 / m) * np.dot(X.T, loss)  # Gradient of cost function\n",
        "\n",
        "        # Step 4: Updating Values of W using Gradient\n",
        "        W_update = W - alpha * dw  # Update rule: W = W - alpha * dw\n",
        "\n",
        "        # Step 5: New Cost Value (Mean Squared Error)\n",
        "        cost = (1 / (2 * m)) * np.sum(loss ** 2)  # Cost function (MSE)\n",
        "        cost_history[iteration] = cost  # Store cost value\n",
        "\n",
        "        # Update W for the next iteration\n",
        "        W = W_update\n",
        "\n",
        "    return W_update, cost_history\n",
        "\n",
        "# Run gradient descent\n",
        "W_final, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "\n",
        "# Display the results\n",
        "print(\"Final Weights (W):\")\n",
        "print(W_final)\n",
        "\n",
        "print(\"\\nCost History (last 10 values):\")\n",
        "print(cost_history[-10:])\n"
      ],
      "metadata": {
        "id": "hSVMQ8Cv-lff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fbed9f6-8974-4e23-904b-58779a5c6095"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights (W):\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "\n",
            "Cost History (last 10 values):\n",
            "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-995e681f8677>:64: RuntimeWarning: overflow encountered in square\n",
            "  cost = (1 / (2 * m)) * np.sum(loss ** 2)  # Cost function (MSE)\n",
            "<ipython-input-7-995e681f8677>:61: RuntimeWarning: invalid value encountered in subtract\n",
            "  W_update = W - alpha * dw  # Update rule: W = W - alpha * dw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate random test data\n",
        "np.random.seed(0) # For reproducibility\n",
        "X = np.random.rand(100, 3) # 100 samples, 3 features\n",
        "Y = np.random.rand(100)\n",
        "W = np.random.rand(3) # Initial guess for parameters\n",
        "# Set hyperparameters\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "# Test the gradient_descent function\n",
        "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "# Print the final parameters and cost history\n",
        "print(\"Final Parameters:\", final_params)\n",
        "print(\"Cost History:\", cost_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W81Yh-sM_KL5",
        "outputId": "4297d172-a114-4e80-c59d-e9434448ff7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Parameters: [0.20551667 0.54295081 0.10388027]\n",
            "Cost History: [0.10788797459582472, 0.10711197094660153, 0.10634880599939901, 0.10559826315680618, 0.10486012948320558, 0.1041341956428534, 0.10342025583900626, 0.1027181077540776, 0.1020275524908062, 0.10134839451441931, 0.1006804415957737, 0.1000235047554587, 0.09937739820884377, 0.09874193931205609, 0.09811694850887098, 0.09750224927850094, 0.0968976680842672, 0.09630303432313951, 0.09571818027612913, 0.09514294105952065, 0.09457715457692842, 0.09402066147216397, 0.09347330508290017, 0.09293493139511913, 0.09240538899833017, 0.09188452904154543, 0.0913722051899995, 0.09086827358260123, 0.09037259279010502, 0.08988502377398919, 0.08940542984603007, 0.08893367662855953, 0.08846963201539432, 0.08801316613342668, 0.08756415130486386, 0.08712246201010665, 0.08668797485125508, 0.08626056851623207, 0.08584012374351278, 0.08542652328745133, 0.08501965188419301, 0.0846193962181636, 0.08422564488912489, 0.08383828837978763, 0.08345721902397185, 0.08308233097530582, 0.08271352017645425, 0.08235068432886682, 0.08199372286303817, 0.08164253690927113, 0.08129702926893387, 0.08095710438620353, 0.08062266832028739, 0.08029362871811391, 0.07996989478748553, 0.0796513772706855, 0.07933798841853089, 0.07902964196486459, 0.07872625310147845, 0.07842773845346054, 0.07813401605495938, 0.0778450053253578, 0.0775606270458499, 0.07728080333641404, 0.07700545763317514, 0.07673451466614989, 0.07646790043736812, 0.07620554219936448, 0.07594736843403344, 0.07569330883184205, 0.07544329427139428, 0.07519725679934074, 0.07495512961062821, 0.07471684702908327, 0.07448234448832412, 0.0742515585129952, 0.07402442670031911, 0.0738008877019607, 0.07358088120619749, 0.0733643479203919, 0.07315122955375959, 0.07294146880042966, 0.07273500932279067, 0.07253179573511871, 0.07233177358748233, 0.0721348893499193, 0.07194109039688139, 0.07175032499194182, 0.07156254227276149, 0.07137769223630935, 0.07119572572433286, 0.07101659440907385, 0.07084025077922623, 0.070666648126131, 0.07049574053020462, 0.07032748284759716, 0.07016183069707572, 0.0699987404471299, 0.06983816920329523, 0.06968007479569092, 0.06952441576676843, 0.06937115135926715, 0.06922024150437375, 0.06907164681008185, 0.06892532854974835, 0.0687812486508435, 0.06863936968389095, 0.06849965485159508, 0.06836206797815195, 0.06822657349874123, 0.06809313644919561, 0.067961722455845, 0.06783229772553254, 0.06770482903579932, 0.06757928372523506, 0.06745562968399212, 0.06733383534445969, 0.06721386967209597, 0.06709570215641501, 0.06697930280212627, 0.06686464212042395, 0.06675169112042348, 0.0666404213007429, 0.06653080464122665, 0.06642281359480932, 0.06631642107951677, 0.06621160047060279, 0.06610832559281864, 0.06600657071281309, 0.0659063105316614, 0.06580752017752023, 0.06571017519840698, 0.06561425155510119, 0.06551972561416586, 0.06542657414108709, 0.06533477429352925, 0.06524430361470467, 0.06515514002685512, 0.06506726182484374, 0.06498064766985515, 0.06489527658320228, 0.06481112794023773, 0.06472818146436811, 0.0646464172211699, 0.06456581561260431, 0.06448635737133043, 0.0644080235551142, 0.06433079554133217, 0.06425465502156798, 0.06417958399630046, 0.06410556476968135, 0.06403257994440141, 0.0639606124166433, 0.06388964537111992, 0.06381966227619645, 0.06375064687909507, 0.06368258320118077, 0.06361545553332655, 0.06354924843135755, 0.06348394671157162, 0.06341953544633615, 0.06335599995975896, 0.06329332582343267, 0.06323149885225086, 0.06317050510029515, 0.06311033085679153, 0.06305096264213547, 0.06299238720398384, 0.0629345915134133, 0.06287756276114324, 0.06282128835382297, 0.0627657559103815, 0.06271095325843898, 0.06265686843077901, 0.06260348966188053, 0.06255080538450809, 0.06249880422636036, 0.06244747500677472, 0.06239680673348793, 0.06234678859945137, 0.06229740997970036, 0.0622486604282762, 0.06220052967520031, 0.062153007623499706, 0.062106084346282515, 0.062059750083863094, 0.06201399524093575, 0.06196881038379625, 0.061924186237610215, 0.06188011368372787, 0.0618365837570441, 0.06179358764340313, 0.061751116677047156, 0.06170916233810801, 0.0616677162501414, 0.06162677017770278, 0.061586316023964055, 0.0615463458283708, 0.06150685176433905, 0.06146782613699094, 0.0614292613809287, 0.061391150058046254, 0.06135348485537795, 0.06131625858298352, 0.061279464171868706, 0.06124309467194143, 0.061207143250002184, 0.06117160318776841, 0.06113646787993252, 0.061101730832252524, 0.06106738565967507, 0.06103342608449018, 0.06099984593451716, 0.06096663914132128, 0.0609337997384604, 0.0609013218597616, 0.06086919973762659, 0.06083742770136588, 0.06080600017556133, 0.06077491167845612, 0.06074415682037193, 0.06071373030215326, 0.060683626913637524, 0.06065384153215141, 0.06062436912103256, 0.0605952047281761, 0.06056634348460599, 0.060537780603070336, 0.060509511376660545, 0.0604815311774538, 0.060453835455178496, 0.06042641973590228, 0.06039927962074216, 0.060372410784596583, 0.060345808974898815, 0.06031947001039151, 0.06029338977992186, 0.06026756424125725, 0.060241989419920934, 0.06021666140804729, 0.0601915763632565, 0.06016673050754826, 0.060142120126214255, 0.06011774156676883, 0.06009359123789796, 0.06006966560842588, 0.06004596120629915, 0.060022474617588105, 0.059999202485504784, 0.059976141509438, 0.05995328844400421, 0.05993064009811483, 0.05990819333405906, 0.059885945066602345, 0.059863892262100066, 0.059842031937626106, 0.059820361160116395, 0.05979887704552664, 0.05977757675800453, 0.05975645750907579, 0.05973551655684408, 0.0597147512052044, 0.05969415880306974, 0.05967373674361096, 0.05965348246350928, 0.05963339344222168, 0.059613467201258485, 0.059593701303473294, 0.05957409335236496, 0.05955464099139111, 0.05953534190329372, 0.059516193809435625, 0.0594971944691485, 0.0594783416790919, 0.05945963327262296, 0.0594410671191769, 0.05942264112365792, 0.05940435322584049, 0.059386201399780576, 0.059368183653237094, 0.059350298027102844, 0.05933254259484533, 0.05931491546195686, 0.05929741476541398, 0.0592800386731462, 0.05926278538351338, 0.05924565312479226, 0.05922864015467154, 0.059211744759755505, 0.059194965255076046, 0.05917829998361292, 0.05916174731582212, 0.059145305649172315, 0.059128973407688926, 0.059112749041506096, 0.05909663102642617, 0.05908061786348662, 0.059064708078534194, 0.05904890022180654, 0.05903319286752055, 0.05901758461346795, 0.05900207408061755, 0.058986659912724324, 0.05897134077594505, 0.058956115358460404, 0.05894098237010357, 0.05892594054199501, 0.05891098862618344, 0.05889612539529293, 0.05888134964217589, 0.05886666017957195, 0.058852055839772675, 0.05883753547429179, 0.058823097953541174, 0.058808742166512155, 0.05879446702046235, 0.058780271440607684, 0.05876615436981961, 0.05875211476832761, 0.05873815161342641, 0.05872426389918856, 0.058710450636181515, 0.05869671085118971, 0.058683043586941104, 0.058669447901838714, 0.05865592286969638, 0.05864246757947903, 0.05862908113504752, 0.05861576265490756, 0.058602511271963004, 0.05858932613327336, 0.058576206399815284, 0.05856315124624814, 0.05855015986068357, 0.05853723144445888, 0.05852436521191438, 0.05851156039017436, 0.0584988162189318, 0.05848613195023677, 0.05847350684828838, 0.058460940189230176, 0.05844843126094919, 0.05843597936287807, 0.05842358380580092, 0.05841124391166213, 0.05839895901337858, 0.05838672845465502, 0.05837455158980245, 0.05836242778355972, 0.05835035641091811, 0.05833833685694878, 0.05832636851663321, 0.05831445079469655, 0.05830258310544367, 0.05829076487259809, 0.05827899552914358, 0.05826727451716845, 0.058255601287712386, 0.0582439753006161, 0.058232396024373266, 0.05822086293598511, 0.058209375520817355, 0.058197933272459756, 0.05818653569258779, 0.05817518229082684, 0.058163872584618616, 0.05815260609908985, 0.058141382366923164, 0.058130200928230166, 0.058119061330426776, 0.058107963128110396, 0.05809690588293942, 0.058085889163514745, 0.058074912545263056, 0.058063975610322414, 0.058053077947429504, 0.05804221915180897, 0.05803139882506447, 0.05802061657507169, 0.0580098720158732, 0.05799916476757483, 0.057988494456244134, 0.057977860713810364, 0.057967263177966154, 0.05795670149207094, 0.05794617530505593, 0.05793568427133074, 0.057925228050691516, 0.057914806308230836, 0.057904418714248757, 0.057894064944165734, 0.05788374467843681, 0.05787345760246728, 0.057863203406529826, 0.05785298178568306, 0.05784279243969133, 0.057832635072946004, 0.05782250939438805, 0.0578124151174319, 0.05780235195989063, 0.057792319643902336, 0.05778231789585793, 0.0577723464463298, 0.05776240503000217, 0.05775249338560223, 0.05774261125583255, 0.05773275838730474, 0.05772293453047407, 0.05771313943957533, 0.0577033728725597, 0.057693634591032654, 0.057683924360193005, 0.05767424194877295, 0.05766458712897906, 0.05765495967643434, 0.057645359370121226, 0.05763578599232564, 0.057626239328581796, 0.05761671916761811, 0.05760722530130401, 0.05759775752459752, 0.057588315635493874, 0.057578899434974906, 0.05756950872695933, 0.05756014331825381, 0.05755080301850501, 0.057541487640152184, 0.05753219699838088, 0.057522930911077144, 0.057513689198782685, 0.05750447168465076, 0.05749527819440267, 0.057486108556285255, 0.05747696260102877, 0.05746784016180581, 0.05745874107419066, 0.05744966517611951, 0.05744061230785123, 0.05743158231192885, 0.05742257503314173, 0.057413590318488285, 0.05740462801713937, 0.057395687980402336, 0.05738677006168561, 0.05737787411646401, 0.057369000002244354, 0.05736014757853204, 0.05735131670679789, 0.057342507250445686, 0.05733371907478018, 0.05732495204697581, 0.057316206036045626, 0.05730748091281111, 0.057298776549872255, 0.05729009282157824, 0.05728142960399854, 0.05727278677489465, 0.05726416421369212, 0.05725556180145319, 0.05724697942084986, 0.0572384169561373, 0.05722987429312795, 0.05722135131916572, 0.05721284792310103, 0.05720436399526589, 0.0571958994274496, 0.057187454112874896, 0.05717902794617434, 0.05717062082336728, 0.057162232641836994, 0.05715386330030848, 0.05714551269882637, 0.05713718073873334, 0.05712886732264895, 0.05712057235444866, 0.05711229573924336, 0.05710403738335914, 0.0570957971943175, 0.05708757508081579, 0.057079370952708035, 0.057071184720986066, 0.05706301629776107, 0.057054865596245175, 0.057046732530733744, 0.05703861701658757, 0.05703051897021566, 0.05702243830905818, 0.057014374951569684, 0.057006328817202634, 0.05699829982639134, 0.05699028790053585, 0.05698229296198646, 0.05697431493402824, 0.05696635374086599, 0.05695840930760929, 0.056950481560257914, 0.0569425704256875, 0.0569346758316353, 0.05692679770668645, 0.05691893598026014, 0.05691109058259633, 0.05690326144474244, 0.05689544849854041, 0.056887651676613984, 0.05687987091235604, 0.056872106139916355, 0.05686435729418944, 0.05685662431080263, 0.0568489071261043, 0.05684120567715239, 0.056833519901703065, 0.05682584973819958, 0.05681819512576124, 0.05681055600417275, 0.056802932313873525, 0.056795323995947306, 0.056787730992111936, 0.05678015324470926, 0.05677259069669528, 0.05676504329163035, 0.05675751097366966, 0.05674999368755382, 0.05674249137859953, 0.05673500399269066, 0.05672753147626912, 0.056720073776326194, 0.05671263084039382, 0.056705202616536096, 0.05669778905334098, 0.05669039009991206, 0.05668300570586036, 0.05667563582129657, 0.056668280396823104, 0.05666093938352648, 0.05665361273296975, 0.056646300397185066, 0.05663900232866641, 0.05663171848036241, 0.05662444880566923, 0.05661719325842369, 0.056609951792896414, 0.05660272436378514, 0.05659551092620811, 0.056588311435697584, 0.05658112584819342, 0.05657395412003692, 0.05656679620796451, 0.05655965206910184, 0.05655252166095763, 0.05654540494141801, 0.056538301868740586, 0.05653121240154893, 0.056524136498826864, 0.056517074119913024, 0.05651002522449555, 0.05650298977260663, 0.05649596772461748, 0.056488959041233064, 0.05648196368348717, 0.05647498161273735, 0.0564680127906601, 0.056461057179246134, 0.05645411474079556, 0.05644718543791332, 0.05644026923350467, 0.056433366090770515, 0.05642647597320331, 0.05641959884458242, 0.05641273466897008, 0.05640588341070717, 0.05639904503440896, 0.05639221950496121, 0.05638540678751615, 0.05637860684748858, 0.056371819650551894, 0.05636504516263454, 0.05635828334991603, 0.05635153417882347, 0.05634479761602789, 0.05633807362844066, 0.05633136218321008, 0.056324663247717996, 0.05631797678957624, 0.05631130277662355, 0.05630464117692215, 0.056297991958754665, 0.05629135509062089, 0.056284730541234666, 0.05627811827952098, 0.05627151827461283, 0.0562649304958483, 0.05625835491276777, 0.05625179149511093, 0.05624524021281401, 0.05623870103600713, 0.05623217393501149, 0.05622565888033667, 0.05621915584267811, 0.05621266479291449, 0.056206185702105234, 0.05619971854148787, 0.05619326328247578, 0.05618681989665565, 0.05618038835578517, 0.05617396863179063, 0.05616756069676472, 0.05616116452296419, 0.05615478008280768, 0.05614840734887347, 0.05614204629389748, 0.05613569689077096, 0.0561293591125386, 0.056123032932396344, 0.05611671832368947, 0.05611041525991056, 0.05610412371469758, 0.056097843661832, 0.05609157507523687, 0.05608531792897493, 0.05607907219724691, 0.056072837854389615, 0.05606661487487427, 0.05606040323330473, 0.056054202904415734, 0.05604801386307135, 0.056041836084263226, 0.056035669543109, 0.056029514214850695, 0.05602337007485319, 0.05601723709860266, 0.05601111526170498, 0.05600500453988435, 0.05599890490898177, 0.05599281634495359, 0.055986738823870105, 0.055980672321914095, 0.055974616815379595, 0.055968572280670384, 0.055962538694298715, 0.05595651603288404, 0.05595050427315166, 0.0559445033919315, 0.05593851336615685, 0.05593253417286313, 0.05592656578918666, 0.05592060819236354, 0.05591466135972839, 0.05590872526871329, 0.05590279989684662, 0.05589688522175184, 0.055890981221146614, 0.05588508787284151, 0.055879205154739084, 0.05587333304483278, 0.05586747152120588, 0.055861620562030555, 0.05585578014556684, 0.05584995025016163, 0.05584413085424776, 0.05583832193634303, 0.0558325234750493, 0.0558267354490515, 0.05582095783711688, 0.05581519061809395, 0.05580943377091164, 0.055803687274578614, 0.05579795110818212, 0.05579222525088745, 0.055786509681936956, 0.05578080438064925, 0.05577510932641848, 0.05576942449871346, 0.055763749877076975, 0.05575808544112503, 0.05575243117054599, 0.05574678704509999, 0.05574115304461813, 0.055735529149001775, 0.05572991533822185, 0.0557243115923182, 0.05571871789139883, 0.05571313421563932, 0.05570756054528211, 0.055701996860635865, 0.055696443142074864, 0.055690899370038335, 0.05568536552502988, 0.05567984158761686, 0.055674327538429685, 0.05566882335816142, 0.055663329027567085, 0.055657844527463085, 0.05565236983872668, 0.05564690494229538, 0.0556414498191665, 0.05563600445039652, 0.055630568817100635, 0.05562514290045211, 0.05561972668168197, 0.05561432014207832, 0.05560892326298588, 0.055603536025805575, 0.055598158411993996, 0.05559279040306291, 0.055587431980578784, 0.055582083126162425, 0.05557674382148841, 0.055571414048284674, 0.05556609378833211, 0.055560783023464094, 0.05555548173556606, 0.055550189906575106, 0.05554490751847952, 0.055539634553318486, 0.05553437099318153, 0.055529116820208266, 0.05552387201658793, 0.055518636564558965, 0.05551341044640875, 0.05550819364447313, 0.05550298614113609, 0.05549778791882936, 0.05549259896003212, 0.05548741924727061, 0.05548224876311773, 0.055477087490192804, 0.0554719354111612, 0.055466792508733924, 0.05546165876566746, 0.055456534164763226, 0.05545141868886745, 0.05544631232087083, 0.05544121504370806, 0.055436126840357744, 0.055431047693841926, 0.05542597758722593, 0.055420916503617974, 0.05541586442616892, 0.055410821338071965, 0.05540578722256242, 0.05540076206291734, 0.055395745842455345, 0.05539073854453634, 0.05538574015256118, 0.05538075064997147, 0.055375770020249314, 0.05537079824691705, 0.05536583531353694, 0.05536088120371103, 0.05535593590108084, 0.05535099938932713, 0.055346071652169704, 0.0553411526733671, 0.05533624243671647, 0.05533134092605322, 0.055326448125250914, 0.05532156401822096, 0.05531668858891247, 0.055311821821311946, 0.055306963699443185, 0.05530211420736701, 0.055297273329180996, 0.05529244104901939, 0.0552876173510529, 0.05528280221948834, 0.05527799563856866, 0.055273197592572584, 0.05526840806581448, 0.055263627042644155, 0.055258854507446706, 0.05525409044464235, 0.05524933483868614, 0.05524458767406786, 0.05523984893531189, 0.055235118606976955, 0.05523039667365596, 0.05522568311997589, 0.055220977930597534, 0.055216281090215466, 0.05521159258355773, 0.055206912395385714, 0.055202240510494154, 0.05519757691371069, 0.055192921589895944, 0.055188274523943294, 0.05518363570077869, 0.05517900510536053, 0.05517438272267951, 0.055169768537758505, 0.055165162535652366, 0.055160564701447826, 0.05515597502026334, 0.055151393477248956, 0.05514682005758613, 0.055142254746487665, 0.05513769752919755, 0.0551331483909908, 0.055128607317173346, 0.055124074293081866, 0.055119549304083755, 0.05511503233557687, 0.05511052337298953, 0.05510602240178027, 0.05510152940743782, 0.05509704437548093, 0.05509256729145828, 0.05508809814094827, 0.05508363690955906, 0.05507918358292834, 0.05507473814672324, 0.055070300586640204, 0.05506587088840496, 0.05506144903777225, 0.05505703502052585, 0.055052628822478474, 0.05504823042947156, 0.05504383982737522, 0.05503945700208816, 0.05503508193953754, 0.055030714625678864, 0.05502635504649593, 0.05502200318800065, 0.055017659036233034, 0.055013322577261034, 0.055008993797180404, 0.05500467268211479, 0.05500035921821539, 0.054996053391661005, 0.05499175518865794, 0.05498746459543984, 0.054983181598267664, 0.0549789061834296, 0.054974638337240846, 0.05497037804604376, 0.0549661252962075, 0.054961880074128146, 0.0549576423662285, 0.054953412158958034, 0.054949189438792796, 0.05494497419223534, 0.05494076640581463, 0.05493656606608597, 0.05493237315963089, 0.05492818767305708, 0.05492400959299837, 0.05491983890611449, 0.05491567559909121, 0.05491151965864005, 0.05490737107149833, 0.05490322982442909, 0.054899095904220915, 0.05489496929768798, 0.05489084999166989, 0.05488673797303166, 0.054882633228663574, 0.05487853574548118, 0.054874445510425175, 0.05487036251046136, 0.054866286732580524, 0.05486221816379847, 0.05485815679115577, 0.0548541026017179, 0.054850055582575, 0.05484601572084191, 0.0548419830036581, 0.05483795741818747, 0.05483393895161846, 0.0548299275911639, 0.054825923324060916, 0.05482192613757092, 0.05481793601897949, 0.05481395295559637, 0.05480997693475535, 0.05480600794381422, 0.0548020459701547, 0.054798091001182415, 0.05479414302432678, 0.054790202027040935, 0.05478626799680179, 0.05478234092110976, 0.05477842078748891, 0.0547745075834868, 0.054770601296674395, 0.05476670191464608, 0.05476280942501958, 0.054758923815435796, 0.05475504507355896, 0.05475117318707636, 0.0547473081436984, 0.05474344993115854, 0.054739598537213205, 0.054735753949641676, 0.05473191615624621, 0.05472808514485178, 0.054724260903306156, 0.05472044341947975, 0.05471663268126573, 0.05471282867657969, 0.0547090313933599, 0.05470524081956701, 0.05470145694318413, 0.05469767975221677, 0.054693909234692674, 0.05469014537866194, 0.05468638817219683, 0.05468263760339178, 0.05467889366036329, 0.05467515633125, 0.05467142560421251, 0.05466770146743334, 0.054663983909116975, 0.054660272917489705, 0.05465656848079964, 0.05465287058731666, 0.05464917922533233, 0.05464549438315985, 0.054641816049134054, 0.05463814421161133, 0.05463447885896955, 0.05463081997960807, 0.05462716756194763, 0.05462352159443039, 0.054619882065519716, 0.054616248963700355, 0.05461262227747823, 0.05460900199538041, 0.054605388105955124, 0.054601780597771675, 0.05459817945942039, 0.05459458467951261, 0.05459099624668059, 0.05458741414957748, 0.0545838383768773, 0.054580268917274875, 0.05457670575948582, 0.05457314889224635, 0.054569598304313474, 0.0545660539844648, 0.054562515921498494, 0.05455898410423328, 0.054555458521508345, 0.05455193916218337, 0.05454842601513845, 0.05454491906927401, 0.05454141831351079, 0.054537923736789846, 0.054534435328072464, 0.0545309530763401, 0.054527476970594374, 0.054524006999857044, 0.054520543153169884, 0.05451708541959473, 0.054513633788213396, 0.054510188248127645, 0.05450674878845912, 0.05450331539834934, 0.054499888066959656, 0.05449646678347117, 0.054493051537084725, 0.05448964231702087, 0.05448623911251984, 0.05448284191284145, 0.054479450707265106, 0.05447606548508972, 0.054472686235633755, 0.054469312948235114, 0.0544659456122511, 0.05446258421705838, 0.05445922875205301, 0.05445587920665035, 0.05445253557028493, 0.05444919783241064, 0.05444586598250044, 0.054442540010046496, 0.05443921990456002, 0.0544359056555714, 0.054432597252629965, 0.05442929468530409, 0.054425997943181044, 0.05442270701586708, 0.05441942189298729, 0.05441614256418564, 0.05441286901912488, 0.05440960124748651, 0.054406339238970806, 0.05440308298329671, 0.054399832470201845, 0.054396587689442416, 0.054393348630793245, 0.05439011528404767, 0.05438688763901759, 0.054383665685533336, 0.0543804494134437, 0.054377238812615865, 0.05437403387293539, 0.054370834584306166, 0.05436764093665037, 0.054364452919908414, 0.05436127052403898, 0.05435809373901896]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Example dataset\n",
        "data = {\n",
        "    'Math': [48, 62, 79, 76, 59],\n",
        "    'Reading': [68, 81, 80, 83, 64],\n",
        "    'Writing': [63, 72, 78, 79, 62]\n",
        "}\n",
        "\n",
        "# Convert the dataset to a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Prepare the feature matrix (X) and target vector (Y)\n",
        "X = df[['Math', 'Reading']].values  # Features: Math and Reading scores\n",
        "Y = df[['Writing']].values          # Target: Writing scores\n",
        "\n",
        "# Add a column of ones for the bias term in X\n",
        "X = np.c_[np.ones(X.shape[0]), X]    # Adding bias (intercept term)\n",
        "\n",
        "# Initialize weights (W)\n",
        "W = np.zeros((X.shape[1], 1))  # Initializing weights to zero\n",
        "\n",
        "# Gradient Descent Parameters\n",
        "alpha = 0.01  # Learning rate\n",
        "iterations = 1000  # Number of iterations\n",
        "\n",
        "# Gradient Descent Function\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    cost_history = [0] * iterations\n",
        "    m = len(Y)\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        Y_pred = np.dot(X, W)\n",
        "        loss = Y_pred - Y\n",
        "        dw = (1 / m) * np.dot(X.T, loss)\n",
        "        W_update = W - alpha * dw\n",
        "        cost = (1 / (2 * m)) * np.sum(loss ** 2)\n",
        "        cost_history[iteration] = cost\n",
        "        W = W_update\n",
        "\n",
        "    return W, cost_history\n",
        "\n",
        "# R Function\n",
        "def r2(Y, Y_pred):\n",
        "    mean_y = np.mean(Y)\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "    r2 = 1 - (ss_res / ss_tot)\n",
        "    return r2\n",
        "\n",
        "# Run gradient descent to optimize weights\n",
        "W_final, _ = gradient_descent(X, Y, W, alpha, iterations)\n",
        "\n",
        "# Make predictions using the optimized weights\n",
        "Y_pred = np.dot(X, W_final)\n",
        "\n",
        "# Calculate R\n",
        "r2_value = r2(Y, Y_pred)\n",
        "\n",
        "# Print the R value\n",
        "print(\"R Value:\", r2_value)\n"
      ],
      "metadata": {
        "id": "BC_9f9-D_hRv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5241fd4d-32c8-4083-9349-ced71a2d8453"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R Value: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-1d3035722f7c>:38: RuntimeWarning: overflow encountered in square\n",
            "  cost = (1 / (2 * m)) * np.sum(loss ** 2)\n",
            "<ipython-input-8-1d3035722f7c>:37: RuntimeWarning: invalid value encountered in subtract\n",
            "  W_update = W - alpha * dw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step -5- Main Function to Integrate All Steps:"
      ],
      "metadata": {
        "id": "jgPzS75PuUtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"Performs gradient descent to optimize weights.\"\"\"\n",
        "    m = len(Y)\n",
        "    cost_history = []\n",
        "    for i in range(iterations):\n",
        "        Y_pred = np.dot(X, W)\n",
        "        error = Y_pred - Y\n",
        "        cost = (1 / (2 * m)) * np.sum(error**2)\n",
        "        # Stop if cost becomes invalid\n",
        "        if np.isnan(cost) or np.isinf(cost):\n",
        "            print(f\"Gradient Descent stopped at iteration {i} due to invalid cost.\")\n",
        "            break\n",
        "        cost_history.append(cost)\n",
        "        gradient = (1 / m) * np.dot(X.T, error)\n",
        "        W -= alpha * gradient\n",
        "    return W, cost_history\n",
        "\n",
        "def rmse(Y_true, Y_pred):\n",
        "    \"\"\"Calculates the Root Mean Squared Error.\"\"\"\n",
        "    return np.sqrt(mean_squared_error(Y_true, Y_pred))\n",
        "\n",
        "def r2(Y_true, Y_pred):\n",
        "    \"\"\"Calculates the R-Squared value.\"\"\"\n",
        "    return r2_score(Y_true, Y_pred)\n",
        "\n",
        "def main():\n",
        "    # Step 1: Load the dataset\n",
        "    data = pd.read_csv('/content/drive/MyDrive/student.csv')\n",
        "\n",
        "    # Step 2: Split the data into features (X) and target (Y)\n",
        "    X = data[['Math', 'Reading']].values  # Features: Math and Reading marks\n",
        "    Y = data['Writing'].values            # Target: Writing marks\n",
        "\n",
        "    # Normalize the features for numerical stability\n",
        "    X_mean = X.mean(axis=0)\n",
        "    X_std = X.std(axis=0)\n",
        "    X = (X - X_mean) / X_std\n",
        "\n",
        "    # Step 3: Split the data into training and test sets (80% train, 20% test)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Add a bias term (intercept) to the feature matrix\n",
        "    X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
        "    X_test = np.c_[np.ones(X_test.shape[0]), X_test]\n",
        "\n",
        "    # Step 4: Initialize weights (W), learning rate, and number of iterations\n",
        "    W = np.zeros(X_train.shape[1])  # Initialize weights\n",
        "    alpha = 0.0001                  # Learning rate\n",
        "    iterations = 1000               # Number of iterations for gradient descent\n",
        "\n",
        "    # Step 5: Perform Gradient Descent\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "\n",
        "    # Step 6: Make predictions on the test set\n",
        "    Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "    # Step 7: Evaluate the model using RMSE and R-Squared\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    # Step 8: Output the results\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "    print(\"RMSE on Test Set:\", model_rmse)\n",
        "    print(\"R-Squared on Test Set:\", model_r2)\n",
        "\n",
        "    # Experiment with different learning rates\n",
        "    for lr in [0.00001, 0.001, 0.1]:\n",
        "        print(f\"\\nExperimenting with Learning Rate: {lr}\")\n",
        "        W_temp, _ = gradient_descent(X_train, Y_train, np.zeros(X_train.shape[1]), lr, iterations)\n",
        "        if np.isnan(W_temp).any():\n",
        "            print(\"Divergence occurred. Adjust the learning rate.\")\n",
        "            continue\n",
        "        Y_pred_temp = np.dot(X_test, W_temp)\n",
        "        print(f\"RMSE: {rmse(Y_test, Y_pred_temp)}, R: {r2(Y_test, Y_pred_temp)}\")\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi2mydFVw9ds",
        "outputId": "3cc8d8f5-94d6-468d-b349-f28a0584a038"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [6.53444381 1.06484994 1.39726956]\n",
            "Cost History (First 10 iterations): [2471.69875, 2471.1904799229724, 2470.6823173626344, 2470.1742622954234, 2469.666314697784, 2469.1584745461646, 2468.65074181702, 2468.14311648681, 2467.6355985320006, 2467.1281879290623]\n",
            "RMSE on Test Set: 63.389626601564274\n",
            "R-Squared on Test Set: -15.05256659905562\n",
            "\n",
            "Experimenting with Learning Rate: 1e-05\n",
            "RMSE: 69.48175118983647, R: -18.28633049576695\n",
            "\n",
            "Experimenting with Learning Rate: 0.001\n",
            "RMSE: 26.083382498908072, R: -1.7179152692809585\n",
            "\n",
            "Experimenting with Learning Rate: 0.1\n",
            "RMSE: 4.788030542881429, R: 0.9084155129155149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model's performance is acceptable, with an RMSE of ~4.8 and R of ~0.91, indicating it explains 91% of the variance in writing scores. The model does not overfit or underfit, as training and test errors are consistent. Experimenting with learning rates showed that a lower rate (0.00001) led to slower convergence and slightly worse performance, while a higher rate (0.1) caused divergence due to instability. A rate of 0.0001 balanced stability and convergence well. Normalizing features was critical for effective gradient descent, and the model generalizes well to the test data."
      ],
      "metadata": {
        "id": "BSyJk1ENxxAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Gradient Descent Function\n",
        "def gradient_descent(X, Y, W, learning_rate, iterations):\n",
        "    m = len(Y)\n",
        "    for i in range(iterations):\n",
        "        # Calculate predictions\n",
        "        Y_pred = np.dot(X, W)\n",
        "        # Compute gradients\n",
        "        gradients = -(2/m) * np.dot(X.T, (Y - Y_pred))\n",
        "        # Update weights\n",
        "        W -= learning_rate * gradients\n",
        "    return W\n",
        "\n",
        "# Model Pipeline Function\n",
        "def model_pipeline(file_path, target_column, learning_rate=0.01, iterations=1000):\n",
        "    \"\"\"\n",
        "    This function loads the dataset, splits it, applies gradient descent, and evaluates the model.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path: str, path to the CSV file containing the dataset.\n",
        "    - target_column: str, the name of the target column in the dataset.\n",
        "    - learning_rate: float, the learning rate for gradient descent.\n",
        "    - iterations: int, number of iterations for gradient descent.\n",
        "    \"\"\"\n",
        "    # Step 1: Load the data\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # Step 2: Handle missing values\n",
        "    # Check if there are any NaN values\n",
        "    print(\"NaN values in each column before cleaning:\")\n",
        "    print(data.isna().sum())\n",
        "\n",
        "    # Option 1: Drop rows with NaN values (if any)\n",
        "    data = data.dropna()\n",
        "\n",
        "    # Verify no NaN values remain\n",
        "    print(\"NaN values in each column after cleaning:\")\n",
        "    print(data.isna().sum())\n",
        "\n",
        "    # Step 3: Split the data into features (X) and target (Y)\n",
        "    X = data.drop(columns=[target_column]).values  # Feature matrix\n",
        "    Y = data[target_column].values                # Target vector\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Step 4: Add intercept term (bias) to the feature matrix\n",
        "    X_train = np.c_[np.ones(X_train.shape[0]), X_train]  # Add intercept term (bias)\n",
        "    X_test = np.c_[np.ones(X_test.shape[0]), X_test]     # Add intercept term (bias)\n",
        "\n",
        "    # Check for NaN values in X_train and X_test after adding intercept term\n",
        "    print(\"NaN values in X_train after adding intercept term:\", np.any(np.isnan(X_train)))\n",
        "    print(\"NaN values in X_test after adding intercept term:\", np.any(np.isnan(X_test)))\n",
        "\n",
        "    # Step 5: Define the weight matrix (W) and initialize learning rate and iterations\n",
        "    W = np.zeros(X_train.shape[1])  # Initialize weights to zeros\n",
        "\n",
        "    # Step 6: Call the gradient descent function to learn the parameters\n",
        "    W = gradient_descent(X_train, Y_train, W, learning_rate, iterations)\n",
        "\n",
        "    # Step 7: Evaluate the model using RMSE and R\n",
        "    Y_pred_train = np.dot(X_train, W)\n",
        "    Y_pred_test = np.dot(X_test, W)\n",
        "\n",
        "    # Calculate RMSE and R for training and testing data\n",
        "    rmse_train = np.sqrt(mean_squared_error(Y_train, Y_pred_train))\n",
        "    rmse_test = np.sqrt(mean_squared_error(Y_test, Y_pred_test))\n",
        "    r2_train = r2_score(Y_train, Y_pred_train)\n",
        "    r2_test = r2_score(Y_test, Y_pred_test)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Training RMSE: {rmse_train}\")\n",
        "    print(f\"Test RMSE: {rmse_test}\")\n",
        "    print(f\"Training R: {r2_train}\")\n",
        "    print(f\"Test R: {r2_test}\")\n",
        "\n",
        "    return W\n",
        "\n",
        "# Example Usage\n",
        "file_path = '/content/drive/MyDrive/student.csv'  # Path to the CSV file\n",
        "target_column = 'Writing'       # The target column (adjust as per your dataset)\n",
        "\n",
        "# Run the model pipeline\n",
        "model_pipeline(file_path, target_column)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "oTuRgw0e2HU2",
        "outputId": "8c730a54-1651-4efd-f64b-e2f4e5f410cf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN values in each column before cleaning:\n",
            "Math       0\n",
            "Reading    0\n",
            "Writing    0\n",
            "dtype: int64\n",
            "NaN values in each column after cleaning:\n",
            "Math       0\n",
            "Reading    0\n",
            "Writing    0\n",
            "dtype: int64\n",
            "NaN values in X_train after adding intercept term: False\n",
            "NaN values in X_test after adding intercept term: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-f43987cba889>:15: RuntimeWarning: invalid value encountered in subtract\n",
            "  W -= learning_rate * gradients\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input contains NaN.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f43987cba889>\u001b[0m in \u001b[0;36m<cell line: 86>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# Run the model pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mmodel_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-f43987cba889>\u001b[0m in \u001b[0;36mmodel_pipeline\u001b[0;34m(file_path, target_column, learning_rate, iterations)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Calculate RMSE and R for training and testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mrmse_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mrmse_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mr2_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     _, y_true, y_pred, sample_weight, multioutput = (\n\u001b[0;32m--> 565\u001b[0;31m         _check_reg_targets_with_floating_dtype(\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets_with_floating_dtype\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mdtype_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_matching_floating_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m   1108\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
          ]
        }
      ]
    }
  ]
}